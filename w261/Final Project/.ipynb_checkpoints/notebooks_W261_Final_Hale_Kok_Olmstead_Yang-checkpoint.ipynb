{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W261 Final Project\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 201`__ \n",
    "\n",
    "\n",
    "Alla Hale, Armand Kok, Daniel Olmstead, Adam Yang\n",
    "\n",
    "The analysis below is a Click Through Rate prediction on the Criteo advertising data made public as part of a [Kaggle competition](https://www.kaggle.com/c/criteo-display-ad-challenge) in 2014.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, avg, struct, array\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, StandardScaler, VectorIndexer, Normalizer\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, BinaryLogisticRegressionSummary\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to making money off the internet, few things drive revenue like display advertisement. Getting the right product in front of the right people can be beneficial to the brand and consumer alike, but to do so is no easy task. Criteo works with over 4,000 e-commerce companies around the world and utilizes an algorithmic machine learning approach on an endless stream of user and advertisement data in an attempt to show the right ads to any given user. As an extension of this goal, CriteoLabs had shared a week's worth of data as a machine learning challenge to develop an algorithm which can accurately predict the click-through-rate. The click-through-rate simply describes the probability that a given user on a given webpage, would click on a given ad. The idea of a click-through-rate can be further expressed by looking at the data provided by CriteoLabs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteo labs provided a week's worth of data where each row of data contains tab delimited values where the first value represents the actual label where 1 means the user clicked on the provided advertisement and 0 means the user did not click on the provided advertisement. Then we are provided with 13 integer columns that mostly represent count features as well as 26 columns that represent categorical features. For anonymization purposes, the values of these categorical features have been hashed onto 32 bits. We are not told what each of the 39 features represent because Criteo would like to keep their feature selection a secret. However, it is implied that together, the 39 features represent a certain user, the web page that the user is on, as well as a certain ad that the user is exposed to. With these 39 features, our goal is to come up with a machine learning algorithm in order to predict the probability that the ad will be clicked by the user on that webpage (click-through-rate). Along the development phase of our machine learning algorithm, we will be highlighting the following course concepts that was relevant to this task: scalability, bias/variance tradeoff, and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4364M  100 4364M    0     0  19.7M      0  0:03:40  0:03:40 --:--:-- 21.1M 0     0  19.8M      0  0:03:39  0:02:33  0:01:06 21.0M\n"
     ]
    }
   ],
   "source": [
    "# Download the data to cluster\n",
    "!curl https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz --output data/dac.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "readme.txt\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "test.txt\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "train.txt\n"
     ]
    }
   ],
   "source": [
    "# Extract the files on the cluster\n",
    "!tar -xvzf data/dac.tar.gz --directory /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dac.tar.gz  readme.txt\ttest.txt  train.txt\n"
     ]
    }
   ],
   "source": [
    "# Check if the extracted files are there\n",
    "!ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://train.txt [Content-Type=text/plain]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "| [1 files][ 10.4 GiB/ 10.4 GiB]   46.6 MiB/s                                   \n",
      "Operation completed over 1 objects/10.4 GiB.                                     \n",
      "Copying file://readme.txt [Content-Type=text/plain]...\n",
      "/ [1 files][  1.9 KiB/  1.9 KiB]                                                \n",
      "Operation completed over 1 objects/1.9 KiB.                                      \n",
      "Copying file://test.txt [Content-Type=text/plain]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "- [1 files][  1.4 GiB/  1.4 GiB]   63.8 MiB/s                                   \n",
      "Operation completed over 1 objects/1.4 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Move the files to the bucket\n",
    "!gsutil cp train.txt gs://w261-final-hoky/data/\n",
    "!gsutil cp readme.txt gs://w261-final-hoky/data/\n",
    "!gsutil cp test.txt gs://w261-final-hoky/data/    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add sample of data - Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Notation\n",
    "\n",
    "In this work, we will be implementing a logistic regression model to predict the click-through-rate based on the data provided by Criteo Labs.\n",
    "\n",
    "The probability, $p$, of belonging to a given class is given by equation 1, \n",
    "\n",
    "\\begin{equation}\\tag{1}\n",
    "p=\\frac{1}{1+\\exp \\left(-\\mathbf{\\theta}^{T} \\cdot \\mathbf{x}_i\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\theta := \\begin{bmatrix} b \\\\ w \\end{bmatrix}$ and  $\\mathbf{x}_i := \\begin{bmatrix} 1 \\\\ x_i \\end{bmatrix}$, and $\\mathbf{w}$ is the vector of weights, $\\mathbf{x}$ is the vector of observations, and $b$ is the bias term. .\n",
    "\n",
    "To estimate the weights for a logistic regression, we use maximum likelihood estimation and maximize the log likelihood\n",
    "\n",
    "\\begin{equation}\\tag{2}\n",
    "I(\\theta) =\\ln \\prod_{i} P_{i}\\\\\n",
    "I(\\theta) =\\ln \\prod_{i}\\left(\\frac{1}{1+\\exp \\left(-\\mathbf{\\theta}^{T} \\cdot \\mathbf{x}_{i}\\right)}\\right)^{\\frac{1+y_{i}}{2}}\\left(1-\\frac{1}{1+\\exp \\left(-\\mathbf{\\theta}^{T} \\cdot \\mathbf{x}_{i}\\right)}\\right)^{\\frac{1-y_{i}}{2}}, \n",
    "\\end{equation}\n",
    "\n",
    "which is equal to minimizing the log loss function, $I(\\theta)$, in equation 3, where $y_i$ is a label.\n",
    "\n",
    "\\begin{equation}\\tag{3}\n",
    "I(\\theta)=\\sum_{i} \\log \\left(1+\\exp \\left(-y_i \\mathbf{\\theta}^{T} \\mathbf{x}_{i}\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Since this is a convex function, we can use gradient descent to find the vector, $\\mathbf{\\theta}$. The gradient is shown in vector notation in equation 4.\n",
    "\n",
    "\\begin{equation}\\tag{4}\n",
    "\\nabla \\mathbf{\\theta}=-\\sum_{i} y_i\\left(1-\\frac{1}{1+\\exp \\left(-y_i\\left(\\mathbf{\\theta}^{T} \\mathbf{x}_{i}\\right)\\right)}\\right) \\cdot \\mathbf{x}_{i}\n",
    "\\end{equation}\n",
    "\n",
    "The vector of weights is initially set to $\\mathbf{0}$, and iteratively updated until convergence, according to equation 5.\n",
    "\n",
    "TODO: Change initial vector value- Adam\n",
    "\n",
    "TODO: Check gradient functions in Spark ML- Alla\n",
    "\n",
    "\\begin{equation}\\tag{5}\n",
    "\\mathbf{\\theta}=\\mathbf{\\theta}-\\eta \\cdot \\nabla \\mathbf{\\theta}\n",
    "\\end{equation}\n",
    "\n",
    "### Regularization\n",
    "\n",
    "To avoid overfitting to our data, we will regularize the gradient descent so that we do not overfit to our data. In order to do this, we introduce a regularization coefficient $\\lambda$ multiplied either by the L1 norm (Lasso) or the L2 norm (Ridge).\n",
    "\n",
    "The objective function and gradient are shown for lasso regression below.\n",
    "\n",
    "Objective function\n",
    "\n",
    "\\begin{equation}\\tag{6}\n",
    "I(W)= \\sum_{i} \\log \\left(1+\\exp \\left(-y_i\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right)\\right)\\right)+\\lambda|\\mathbf{w}|\n",
    "\\end{equation}\n",
    "\n",
    "Gradient\n",
    "\n",
    "\\begin{equation}\\tag{7}\n",
    "\\nabla \\mathbf{w}=-\\sum_{i}y_i\\left(1-\\frac{1}{1+\\exp \\left(-y_i\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right)\\right)}\\right) \\cdot \\mathbf{x}_{i}+\\lambda \\text{sign}(\\mathbf{w})\n",
    "\\end{equation}\n",
    "\n",
    "And again for ridge regression below.\n",
    "\n",
    "Objective function\n",
    "\n",
    "\\begin{equation}\\tag{8}\n",
    "I(W)= \\sum_{i} \\log \\left(1+\\exp \\left(-y_i\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right)\\right)\\right)+\\frac{1}{2}\\lambda \\mathbf{w}^{2}\n",
    "\\end{equation}\n",
    "\n",
    "Gradient\n",
    "\n",
    "\\begin{equation}\\tag{9}\n",
    "\\nabla \\mathbf{w}=-\\sum_{i}y_i\\left(1-\\frac{1}{1+\\exp \\left(-y_i\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right)\\right)}\\right) \\cdot \\mathbf{x}_{i}+\\lambda \\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "In all cases, the update to the weights remains the same as shown in equation 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Toy Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get toy data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in *equation 3*, the Log Loss equation, $I(\\theta)$, is given as:\n",
    "\n",
    "$$\n",
    "I(\\theta)=\\sum_{i} \\log \\left(1+\\exp \\left(-y_i \\mathbf{\\theta}^{T} \\mathbf{x}_{i}\\right)\\right)\n",
    "$$\n",
    "\n",
    "where $\\theta := \\begin{bmatrix} b \\\\ w \\end{bmatrix}$ and  $\\mathbf{x}_i := \\begin{bmatrix} 1 \\\\ x_i \\end{bmatrix}$. Furthermore, $x_i$ represents each row of our feature inputs, $y_i$ represents the labels to each of the rows, and $w$ represents the coefficient vector of our model. The $\\theta$ and $\\bar{x}_i$ notations allow us to incorporate the intercept of the logistic regression model into the loss function. If we leave out the intercept of the model, we will be claiming that when all the predictors are equal to 0, the resulting $logit(p)$ will be 0. Since $logit(p) = log(\\frac{p}{1-p})$, if we set $logit(p) = 0$, we subsequently get: \n",
    "$$log(\\frac{p}{1-p}) = 0$$\n",
    "$$$$\n",
    "$$e^0 = \\frac{p}{1-p} = 1$$\n",
    "$$$$\n",
    "$$\\therefore p = 0.5$$\n",
    "This means that if we leave out the intercept of the model, we will be claiming that when all the predictors are zero, the probability of getting a click on the ad will be 0.5. This is not a claim that we can make so it is very important that we include the intercept into our logistic regression.\n",
    "Below is a function that calculates the Log Loss of a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rest of Adam's hand coded logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EDA and Discussion of Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "TODO: Daniel, clean up and paste in interesting EDA\n",
    "\n",
    "Percent nulls\n",
    "\n",
    "Column stuff\n",
    "\n",
    "Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier\n",
    "\n",
    "TODO: Armand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "- Add daypart\n",
    "\n",
    "- Creation of binarized numerics, scaling, imputing to mean, impute categoricals to null string\n",
    "\n",
    "- Feature selection/optimization (if fruitful)\n",
    "\n",
    "TODO: Daniel, Armand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually implement the logistic regression, we will be taking advantage of Spark ML, a machine learning library based on Spark's dataframes. This library is optimized for parallel computing, and implements the regression in much the same way as we show in the calculations on the toy set, above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML Pipeline on Toy Dataset\n",
    "\n",
    "Let's look at our transformation pipeline on a toy example, `dataset`. This is slightly different from the toy example above, which only contained numerical values.\n",
    "\n",
    "When we one-hot encode our countries, the countries are assigned the following indices: {NZ: 0, CA: 1, US: 2, Unseen: 3}.  The `HandleInvalid` attribute is set to `keep`, which will add the last, `Unseen` category. This way, when we transform a dataset that contains a country that was not in the training set, we will have somewhere to keep the information. \n",
    "\n",
    "To add interactions, we use the RFormula, which creates a list of feature values in the following order: [NZ, CA, US, Unseen, Hour, NZ\\*Hour, CA\\*Hour, US\\*Hour, Unseen\\*Hour]. Here, `hour` should be scaled so that we avoid convergence issues with gradient descent.\n",
    "\n",
    "TODO: Alla- explain all features into single vector.\n",
    "\n",
    "Note, it is also possible to create interaction features simply by using RFormula directly on the country column. Spark ML does the one-hot encoding under the covers. However, in this case, we wanted more control over how to one hot encode (keeping a column for unseen categories, keeping the last column of the features within a field), so we explicitely one hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>US</td>\n",
       "      <td>CA</td>\n",
       "      <td>NZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clicked</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>countryidx</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>countryclassVec</th>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hourclassVec</th>\n",
       "      <td>[18.0]</td>\n",
       "      <td>[12.0]</td>\n",
       "      <td>[15.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hourscaled</th>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[-1.0]</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>features</th>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 18.0, 0.0, 0.0, 18.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 12.0, 0.0, 12.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 15.0, 15.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               0  \\\n",
       "id                                                             7   \n",
       "country                                                       US   \n",
       "hour                                                          18   \n",
       "clicked                                                        1   \n",
       "countryidx                                                     2   \n",
       "countryclassVec                             (0.0, 0.0, 1.0, 0.0)   \n",
       "hourclassVec                                              [18.0]   \n",
       "hourscaled                                                 [1.0]   \n",
       "features         (0.0, 0.0, 1.0, 0.0, 18.0, 0.0, 0.0, 18.0, 0.0)   \n",
       "label                                                          1   \n",
       "\n",
       "                                                               1  \\\n",
       "id                                                             8   \n",
       "country                                                       CA   \n",
       "hour                                                          12   \n",
       "clicked                                                        0   \n",
       "countryidx                                                     1   \n",
       "countryclassVec                             (0.0, 1.0, 0.0, 0.0)   \n",
       "hourclassVec                                              [12.0]   \n",
       "hourscaled                                                [-1.0]   \n",
       "features         (0.0, 1.0, 0.0, 0.0, 12.0, 0.0, 12.0, 0.0, 0.0)   \n",
       "label                                                          0   \n",
       "\n",
       "                                                               2  \n",
       "id                                                             9  \n",
       "country                                                       NZ  \n",
       "hour                                                          15  \n",
       "clicked                                                        0  \n",
       "countryidx                                                     0  \n",
       "countryclassVec                             (1.0, 0.0, 0.0, 0.0)  \n",
       "hourclassVec                                              [15.0]  \n",
       "hourscaled                                                 [0.0]  \n",
       "features         (1.0, 0.0, 0.0, 0.0, 15.0, 15.0, 0.0, 0.0, 0.0)  \n",
       "label                                                          0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, create the toy dataset, dataset\n",
    "dataset = spark.createDataFrame(\n",
    "    [(7, \"US\", 18, 1.0),\n",
    "     (8, \"CA\", 12, 0.0),\n",
    "     (9, \"NZ\", 15, 0.0)],\n",
    "    [\"id\", \"country\", \"hour\", \"clicked\"])\n",
    "\n",
    "# Set empty stages\n",
    "stages = []\n",
    "\n",
    "# Pipeline step 1: one hot encoding for the categorical variables\n",
    "# cast each record in in categorical column c to an index\n",
    "c='country'\n",
    "stridx = StringIndexer(inputCol=c, outputCol = c + \"idx\").setHandleInvalid(\"keep\")\n",
    "# one hot encode the indexed categorical column\n",
    "encoder = OneHotEncoderEstimator(inputCols=[stridx.getOutputCol()], outputCols=[c + \"classVec\"]).setDropLast(False)\n",
    "stages += [stridx, encoder]\n",
    "\n",
    "# Pipeline step 2: Standardize the numerical features\n",
    "n='hour'\n",
    "num_assembler = VectorAssembler(inputCols=[n], outputCol=n+\"classVec\")\n",
    "num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=n+\"scaled\", withMean=True)\n",
    "stages += [num_assembler, num_scaler]\n",
    "\n",
    "# Pipeline step 3: create interactions\n",
    "formula = RFormula(\n",
    "    formula=\"clicked ~ countryclassVec + hour + countryclassVec:hour \",\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\")\n",
    "stages +=[formula]\n",
    "\n",
    "# Fill pipeline with stages\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Train the transformations on train data\n",
    "dataset_transformed = pipeline.fit(dataset).transform(dataset)\n",
    "\n",
    "pd.DataFrame(dataset_transformed.collect(), columns=dataset_transformed.columns).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our features vectors and the labels, we can fit a logistic regression with gradient descent, using Spark ML's pipeline. For this example, we are not optimizing the logistic regression hyper parameters.\n",
    "\n",
    "TODO: Alla\n",
    "Describe differences between hand coded example-- Learning rate, regularization, randomize starting vector for gradient descent. \n",
    "\n",
    "Add some discussion of why we use a UDF for log loss calculation (cannot disassemble vector, or access values inside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLogLoss(prob, label):\n",
    "    ''' Calculates the log loss for a single observation\n",
    "    Args:\n",
    "        prob- float, a probability between 0 and 1\n",
    "        label- integer, a label that is either 0 or 1\n",
    "    Output:\n",
    "        logloss- float, the log loss value for the single observation\n",
    "    '''\n",
    "    # for the special case when prob=0 or 1, need a small value to avoid log(0)\n",
    "    prob = prob[int(label)]\n",
    "    eps = 10e-14\n",
    "    if prob == 0:\n",
    "        prob += eps\n",
    "    if prob == 1:\n",
    "        prob -= eps\n",
    "    return -label * np.log(prob) - (1 - label) * np.log(1-prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stages:\n",
      " [LogisticRegression_4038a2b6f73ea9b987a0]\n",
      "... executed pipeline in 0.8796179294586182 seconds\n",
      "The accuracy is 1.0.\n",
      "The f1 score is 1.0.\n",
      "The precision is 1.0.\n",
      "The recall is 1.0.\n",
      "+------------------+\n",
      "|      avg(logloss)|\n",
      "+------------------+\n",
      "|1.3497655590375264|\n",
      "+------------------+\n",
      "\n",
      "... calculated average log loss in 0.4709174633026123 seconds\n"
     ]
    }
   ],
   "source": [
    "stages = []\n",
    "# Pipeline step 5: run the logistic regression\n",
    "lr = LogisticRegression(featuresCol='features', labelCol ='label', regParam=0.5, maxIter=50, fitIntercept=True)\n",
    "stages += [lr]\n",
    "\n",
    "# print(lr.explainParams())\n",
    "\n",
    "print('stages:\\n', stages)\n",
    "\n",
    "# fit the pipeline to do the series of fit/transform defined in stages\n",
    "start = time.time()\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Train the transformations on train data\n",
    "pipelineModelTrain = pipeline.fit(dataset_transformed)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions_df = pipelineModelTrain.transform(dataset_transformed)\n",
    "\n",
    "print(f\"... executed pipeline in {time.time() - start} seconds\")\n",
    "\n",
    "# Evaluate performance\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='label_transformed')\n",
    "evaluatorAccuracy = MulticlassClassificationEvaluator(labelCol='label', metricName='accuracy')\n",
    "print('The accuracy is {}.'.format(evaluatorAccuracy.evaluate(predictions_df)))\n",
    "evaluatorF1 = MulticlassClassificationEvaluator(labelCol='label', metricName='f1')\n",
    "print('The f1 score is {}.'.format(evaluatorF1.evaluate(predictions_df)))\n",
    "evaluatorPrecision = MulticlassClassificationEvaluator(labelCol='label', metricName='weightedPrecision')\n",
    "print('The precision is {}.'.format(evaluatorPrecision.evaluate(predictions_df)))\n",
    "evaluatorRecall = MulticlassClassificationEvaluator(labelCol='label', metricName='weightedRecall')\n",
    "print('The recall is {}.'.format(evaluatorRecall.evaluate(predictions_df)))\n",
    "\n",
    "#TODO: Daniel, can you add your confusion matrix, here?\n",
    "\n",
    "# Calculate logloss\n",
    "start = time.time()\n",
    "loglossUDF = udf(lambda x: float(computeLogLoss(x[0], x[1])), returnType=FloatType())\n",
    "newdf = predictions_df.withColumn('logloss', loglossUDF(struct('probability', 'label')))\n",
    "newdf.select(avg(col(\"logloss\"))).show()\n",
    "print(f\"... calculated average log loss in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Feature Selection\n",
    "Daniel, optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Hyperparameter Tuning (Nice to Have?)\n",
    "\n",
    "At a minimum, discuss the hyperparameters we have. Which regularization, why, (regParam, elasticNetParam)? MaxIter,Standardization, fitIntercept\n",
    "\n",
    "Cross validation\n",
    "Armand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Daniel's Confusion Matrix + metrics code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fitted model had the following parameters:\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term (default: True, current: False)\n",
      "labelCol: label column name (default: label, current: label_transformed)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100, current: 4)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0, current: 15.0)\n",
      "standardization: whether to standardize the training features before fitting the model (default: True)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create ourModel and ourModelSummary\n",
    "ourModel = pipelineModelTrain.stages[-1]\n",
    "ourModelSummary = ourModel.summary\n",
    "print('The fitted model had the following parameters:\\n{}\\n'.format(ourModel.explainParams()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Application of Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through addressing the prediction of click through rates, we have applied many of the course concepts from W261: Machine Learning at Scale. Though there were more concepts addressed in the main body of this report, we will focus here on the three most important: scalability, feature selection, and bias/variance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: Alla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Scalability\n",
    "\n",
    "Dataframes/Spark ML vs. Hand coding with RDDs \n",
    "\n",
    "    Speed of Scala vs. Python\n",
    "    \n",
    "    Spark optimizations\n",
    "    \n",
    "    UDFs vs. built in functions (Scala vs. Python)\n",
    "    \n",
    "    Pandas integration (EDA, print output)\n",
    "    \n",
    "    Pipelining\n",
    "    \n",
    "    Methods, UDFs, SQL syntax\n",
    "    \n",
    "Checkpointing vs. caching to guard against executer failure.\n",
    "\n",
    "    Logical execution plan\n",
    "    \n",
    "    Physical execution plan\n",
    "    \n",
    "    DAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Feature Engineering\n",
    "\n",
    "One-hot encoding\n",
    "\n",
    "Binarizing (present/not present for numerical columns)\n",
    "\n",
    "Too many features to fit all interaction terms, have to select\n",
    "\n",
    "Do not want to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Bias/Variance tradeoff\n",
    "\n",
    "Feature selection\n",
    "\n",
    "Regularization\n",
    "\n",
    "Early stopping (fewer iterations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The great TODO list\n",
    "\n",
    "Here, we will address what we would do if we were to productionalize this code. What's left...\n",
    "\n",
    "Random forest\n",
    "\n",
    "Saving model to file\n",
    "\n",
    "Matrix factorization to fill in nulls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
