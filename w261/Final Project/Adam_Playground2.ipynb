{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W261 Final Project\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 201`__ \n",
    "\n",
    "\n",
    "Alla Hale, Armand Kok, Daniel Olmstead, Adam Yang\n",
    "\n",
    "The analysis below is a Click Through Rate prediction on the Criteo advertising data made public as part of a [Kaggle competition](https://www.kaggle.com/c/criteo-display-ad-challenge) in 2014.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, StandardScaler, VectorIndexer, Normalizer\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, BinaryLogisticRegressionSummary\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw5_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In this work, we will be implementing a logistic regression model to predict the click-through-rate based on the data provided by Criteo Labs.\n",
    "\n",
    "The probability, $p$, of belonging to a given class is given by equation 1, where $\\mathbf{w}$ is the vector of weights, $\\mathbf{x}$ is the vector of observations, and $b$ is the bias term.\n",
    "\n",
    "\\begin{equation}\\tag{1}\n",
    "p=\\frac{1}{1+\\exp \\left(-\\mathbf{w}^{T} \\cdot \\mathbf{x}+b\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "To estimate the weights for a logistic regression, we use maximum likelihood estimation and maximize the log likelihood\n",
    "\n",
    "\\begin{equation}\\tag{2}\n",
    "I(W) =\\ln \\prod_{i} P_{i}\\\\\n",
    "I(W) =\\ln \\prod_{i}\\left(\\frac{1}{1+\\exp \\left(-\\mathbf{w}^{T} \\cdot \\mathbf{x}_{i}+b\\right)}\\right)^{\\frac{1+y_{i}}{2}}\\left(1-\\frac{1}{1+\\exp \\left(-\\mathbf{w}^{T} \\cdot \\mathbf{x}_{i}+b\\right)}\\right)^{\\frac{1-y_{i}}{2}}, \n",
    "\\end{equation}\n",
    "\n",
    "which is equal to minimizing the log loss function, $I(W)$, in equation 3, where $y$ is a label.\n",
    "\n",
    "\\begin{equation}\\tag{3}\n",
    "I(W)=\\sum_{i} \\log \\left(1+\\exp \\left(-y \\mathbf{w}^{T} \\mathbf{x}_{i}\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Since this is a convex function, we can use gradient descent to find the vector, $\\mathbf{w}$. The gradient is shown in vector notation in equation 4.\n",
    "\n",
    "\\begin{equation}\\tag{4}\n",
    "\\nabla \\mathbf{w}=-\\sum_{i} y\\left(1-\\frac{1}{1+\\exp \\left(-y\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right)\\right)}\\right) \\cdot \\mathbf{x}_{i}\n",
    "\\end{equation}\n",
    "\n",
    "The vector of weights is initially set to $\\mathbf{0}$, and iteratively updated until convergence, according to equation 5.\n",
    "\\begin{equation}\\tag{5}\n",
    "\\mathbf{w}=\\mathbf{w}-\\eta \\cdot \\nabla \\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "Equations for the $i^{th}$ component of the vector and the gradient, corresponding to equations 3, 4, and 5, are shown below in equations 6, 7, and 8, respectively.\n",
    "\n",
    "\\begin{equation}\\tag{6}\n",
    "I(W)=\\sum_{i} Y^{l}\\left(w_{0}+\\sum_{i}^{n} w_{i} X_{i}^{l}\\right)-\\ln \\left(1+\\exp \\left(w_{0}+\\sum_{i}^{n} w_{i} X_{i}^{l}\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\\tag{7}\n",
    "\\frac{\\partial l(W)}{\\partial w_{i}}=\\sum_{l} X_{i}^{l}\\left(Y^{l}-\\hat{P}\\left(Y^{l}=1 | X^{l}, W\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\\tag{8}\n",
    "w_{i} \\leftarrow w_{i}+\\eta \\sum_{l} X_{i}^{l}\\left(Y^{l}-\\hat{P}\\left(Y^{l}=1 | X^{l}, W\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Likely, we will regularize the gradient descent so that we do not overfit to our data. In order to do this, we introduce a regularization coefficient $\\lambda$ multiplied either by the L1 norm (Lasso) or the L2 norm (Ridge).\n",
    "\n",
    "The objective function and gradient are shown for lasso regression below.\n",
    "\n",
    "Objective function\n",
    "\n",
    "\\begin{equation}\\tag{9}\n",
    "I(W)= \\sum_{i} \\log \\left(1+\\exp \\left(-y\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right)\\right)\\right)+\\lambda|\\mathbf{w}|\n",
    "\\end{equation}\n",
    "\n",
    "Gradient\n",
    "\n",
    "\\begin{equation}\\tag{10}\n",
    "\\nabla \\mathbf{w}=-\\sum_{i}y\\left(1-\\frac{1}{1+\\exp \\left(-y\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right)\\right)}\\right) \\cdot \\mathbf{x}_{i}+\\lambda \\text{sign}(\\mathbf{w})\n",
    "\\end{equation}\n",
    "\n",
    "And again for ridge regression below.\n",
    "\n",
    "Objective function\n",
    "\n",
    "\\begin{equation}\\tag{11}\n",
    "I(W)= \\sum_{i} \\log \\left(1+\\exp \\left(-y\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right)\\right)\\right)+\\lambda \\mathbf{w}^{2}\n",
    "\\end{equation}\n",
    "\n",
    "Gradient\n",
    "\n",
    "\\begin{equation}\\tag{12}\n",
    "\\nabla \\mathbf{w}=-\\sum_{i}y\\left(1-\\frac{1}{1+\\exp \\left(-y\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right)\\right)}\\right) \\cdot \\mathbf{x}_{i}+\\lambda \\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "In all cases, the update to $\\mathbf{w}$ remains the same as shown in equation 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EDA and Discussion of Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# Define the schema prior to loading the data\n",
    "schema = StructType([StructField(\"label\", IntegerType(), True),\n",
    "                     StructField(\"I1\", IntegerType(), True),\n",
    "                     StructField(\"I2\", IntegerType(), True),\n",
    "                     StructField(\"I3\", IntegerType(), True),\n",
    "                     StructField(\"I4\", IntegerType(), True),\n",
    "                     StructField(\"I5\", IntegerType(), True),\n",
    "                     StructField(\"I6\", IntegerType(), True),\n",
    "                     StructField(\"I7\", IntegerType(), True),\n",
    "                     StructField(\"I8\", IntegerType(), True),\n",
    "                     StructField(\"I9\", IntegerType(), True),\n",
    "                     StructField(\"I10\", IntegerType(), True),\n",
    "                     StructField(\"I11\", IntegerType(), True),\n",
    "                     StructField(\"I12\", IntegerType(), True),\n",
    "                     StructField(\"I13\", IntegerType(), True),\n",
    "                     StructField(\"C1\", StringType(), True),\n",
    "                     StructField(\"C2\", StringType(), True),\n",
    "                     StructField(\"C3\", StringType(), True),\n",
    "                     StructField(\"C4\", StringType(), True),\n",
    "                     StructField(\"C5\", StringType(), True),\n",
    "                     StructField(\"C6\", StringType(), True),\n",
    "                     StructField(\"C7\", StringType(), True),\n",
    "                     StructField(\"C8\", StringType(), True),\n",
    "                     StructField(\"C9\", StringType(), True),\n",
    "                     StructField(\"C10\", StringType(), True),\n",
    "                     StructField(\"C11\", StringType(), True),\n",
    "                     StructField(\"C12\", StringType(), True),\n",
    "                     StructField(\"C13\", StringType(), True),\n",
    "                     StructField(\"C14\", StringType(), True),\n",
    "                     StructField(\"C15\", StringType(), True),\n",
    "                     StructField(\"C16\", StringType(), True),\n",
    "                     StructField(\"C17\", StringType(), True),\n",
    "                     StructField(\"C18\", StringType(), True),\n",
    "                     StructField(\"C19\", StringType(), True),\n",
    "                     StructField(\"C20\", StringType(), True),\n",
    "                     StructField(\"C21\", StringType(), True),\n",
    "                     StructField(\"C22\", StringType(), True),\n",
    "                     StructField(\"C23\", StringType(), True),\n",
    "                     StructField(\"C24\", StringType(), True),\n",
    "                     StructField(\"C25\", StringType(), True),\n",
    "                     StructField(\"C26\", StringType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load toy data into dataframe\n",
    "toy_df = spark.read.parquet(\"toyData/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show toy_data\n",
    "pd.DataFrame(toy_df.take(30), columns=toy_df.columns).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gradient Descent Approach to Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just some code to get only the numeric numbers for training. Won't be using this for the official notebook\n",
    "toy_rdd = toy_df.rdd.map(tuple)\n",
    "toy_RDD = toy_rdd.map(lambda line: (line[0],(line[1:14]))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.createDataFrame(\n",
    "    [(1.0, 1, 1, 1),\n",
    "     (1.0, 5, 2, 2),\n",
    "     (0.0, 3, 0, 3)],\n",
    "    [\"label\", \"page_num\", \"hour\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_rdd = dataset.rdd.map(tuple)\n",
    "toy_RDD = toy_rdd.map(lambda line: (line[0],(line[1:4]))).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Log Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in *equation 3*, the Log Loss equation, $I(W)$, is given as:\n",
    "\n",
    "$$\n",
    "I(W)=\\sum_{i} \\log \\left(1+\\exp \\left(-y_i \\mathbf{\\theta}^{T} \\mathbf{\\bar{x}}_{i}\\right)\\right)\n",
    "$$\n",
    "\n",
    "where $\\theta := \\begin{bmatrix} b \\\\ w \\end{bmatrix}$ and  $\\bar{x}_i := \\begin{bmatrix} 1 \\\\ x_i \\end{bmatrix}$. Furthermore, $x_i$ represents each row of our feature inputs, $y_i$ represents the labels to each of the rows, and $w$ represents the coefficient vector of our model. The $\\theta$ and $\\bar{x}_i$ notations allow us to incorporate the intercept of the logistic regression model into the loss function. If we leave out the intercept of the model, we will be claiming that when all the predictors are equal to 0, the resulting $logit(p)$ will be 0. Since $logit(p) = log(\\frac{p}{1-p})$, if we set $logit(p) = 0$, we subsequently get: \n",
    "$$log(\\frac{p}{1-p}) = 0$$\n",
    "$$e^0 = \\frac{p}{1-p} = 1$$\n",
    "$$\\therefore p = 0.5$$\n",
    "This means that if we leave out the intercept of the model, we will be claiming that when all the predictors are zero, the probability of getting a click on the ad will be 0.5. This is not a claim that we can make so it is very important that we include the intercept into our logistic regression.\n",
    "Below is a function that calculates the Log Loss of a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute the Log Loss of our model.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (y, features_array)\n",
    "        W       - (array) model coefficients\n",
    "    \"\"\"\n",
    "    # Add 1 to the front of the predictors array\n",
    "    # Note that the b value has to be added to the front of our theta array\n",
    "    augmentedData = dataRDD.map(lambda x: (x[0], np.append([1.0], x[1])))\n",
    "\n",
    "    def LogLossPerRow(line):\n",
    "        # Calculate log(1 + exp(-ywx)) for each row of the data in parallel\n",
    "        actual_y, features = line\n",
    "        predicted_y = np.dot(np.transpose(W),features)\n",
    "        yield np.log(1.0 + np.exp(-1.0*actual_y*predicted_y))\n",
    "    \n",
    "    loss = augmentedData.flatMap(LogLossPerRow).sum()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test our LogLoss function by setting b to the mean of the our label values and all our predictors to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of all our label values\n",
    "meanQuality = toy_RDD.map(lambda x: x[0]).mean()\n",
    "print(f\"Mean: {meanQuality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the baseline model so that b is the mean we calculated and all other features are 0\n",
    "#BASELINE = np.append([meanQuality],[0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "BASELINE = np.append([meanQuality],[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the loss of the baseline model\n",
    "assert len(BASELINE) == len(toy_RDD.take(1)[0][1]) + 1, \"Double check model dimensions\"\n",
    "print(f\"Baseline model loss: {LogLoss(toy_RDD, BASELINE)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if our result is correct or not\n",
    "\n",
    "# Count how many 1's and 0's we have in our label\n",
    "ones = toy_RDD.map(lambda line: 1 if line[0] == 1 else 0).sum()\n",
    "zeros = toy_RDD.count() - ones\n",
    "print(\"There are {} ones and {} zeros in the label column of our dataset\".format(ones, zeros))\n",
    "\n",
    "# Calculate the LogLoss in our example case\n",
    "loss = ones*np.log(1.0 + np.exp(-1.0*meanQuality)) + zeros*np.log(1.0 + np.exp(0))\n",
    "print(\"The loss of the baseline model through manual calculation is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results of our LogLoss() function is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to find the Log Loss of our model, we will approach finding the logistic regression model through gradient descent. Equations 4, 10, and 12, gives us the gradients for a vanilla gradient descent, L1 regularization (lasso), and L2 regularization (ridge) respectively. These gradient equations are provided below in the same order.\n",
    "$$\n",
    "\\nabla \\mathbf{w}=-\\sum_{i} y_i\\left(1-\\frac{1}{1+\\exp \\left(-y_i \\mathbf{\\theta}^{T} \\mathbf{\\bar{x}}_{i}\\right)}\\right) \\cdot \\mathbf{x}_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla \\mathbf{w}=-\\sum_{i}y_i\\left(1-\\frac{1}{1+\\exp \\left(-y_i \\mathbf{\\theta}^{T} \\mathbf{\\bar{x}}_{i}\\right)}\\right) \\cdot \\mathbf{x}_{i}+\\lambda \\text{sign}(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla \\mathbf{w}=-\\sum_{i}y_i\\left(1-\\frac{1}{1+\\exp \\left(-y_i \\mathbf{\\theta}^{T} \\mathbf{\\bar{x}}_{i}\\right)}\\right) \\cdot \\mathbf{x}_{i}+\\lambda \\mathbf{w}\n",
    "$$\n",
    "\n",
    "With these gradients, we can iteratively update our model with the following equation until it converges:\n",
    "\n",
    "$$\n",
    "\\mathbf{w_{new}}=\\mathbf{w_{old}}-\\eta \\cdot \\nabla \\mathbf{w}\n",
    "$$\n",
    "\n",
    "The code below will be caculating the gradient, whether it is vanilla, lasso, or ridge, depending on the user's choice. Then the gradient will be used to update the coefficients of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code calculates the 3 different gradients with and without regularization\n",
    "# Then it updates the model (w) and outputs the new model\n",
    "def GDUpdate_wReg(dataRDD, W, learningRate = 0.1, regType = None, regParam = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step/update with ridge or lasso regularization.\n",
    "    Args:\n",
    "        dataRDD - tuple of (y, features_array)\n",
    "        W       - (array) model coefficients with intercept at index 0\n",
    "        learningRate - (float) defaults to 0.1\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient\n",
    "    Returns:\n",
    "        model   - (array) updated coefficients, intercept still at index 0\n",
    "    \"\"\"\n",
    "    # augmented data\n",
    "    augmentedData = dataRDD.map(lambda x: (x[0], np.append([1.0], x[1])))\n",
    "    \n",
    "    new_model = None\n",
    "\n",
    "    def GradientPerRow(line):\n",
    "        # Calculates -y(1- 1/ 1+ exp( -ywx))x for each row\n",
    "        true_y, features = line\n",
    "        predicted_y = np.dot(np.transpose(W),features)\n",
    "        yield -1.0*true_y*(1.0 - 1.0/(1.0 + np.exp(-1.0*true_y*predicted_y)))*features\n",
    "        \n",
    "    # Use the same way as before to find the first component of the gradient function\n",
    "    grad = augmentedData.flatMap(GradientPerRow).sum()\n",
    "    \n",
    "    # Take out the bias stored in index 0 of W\n",
    "    model = W[1:]\n",
    "    \n",
    "    # Figure out the regulation component\n",
    "    if regType == None:\n",
    "        pass\n",
    "        \n",
    "    elif regType == 'lasso':\n",
    "        reg_comp = regParam*np.sign(model)\n",
    "        # Update the gradient function by taking the regularization component into consideration\n",
    "        grad = grad + np.append(0,reg_comp)\n",
    "                \n",
    "    elif regType == 'ridge':\n",
    "        reg_comp = regParam*model\n",
    "        # Update the gradient function by taking the regularization component into consideration\n",
    "        grad = grad + np.append(0,reg_comp)\n",
    "    \n",
    "    new_model = W - (learningRate*grad)\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDUpdate_wReg(toy_RDD, BASELINE, learningRate = 0.1, regType = None, regParam = 0.1)\n",
    "#prob = 1.0/(1.0 + np.exp(-1.0*np.dot(updated_model, np.append(1,pd.DataFrame(toy_df_transformed.collect(), columns=toy_df_transformed.columns)['features'][2]))))\n",
    "#pd.DataFrame(toy_df_transformed.collect(), columns=toy_df_transformed.columns)['features'][0]\n",
    "#prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDUpdate_wReg(toy_RDD, [0.73451539, 0.20354618, 1.01773089, 0.50886545], learningRate = 0.1, regType = None, regParam = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [0.73451539, 0.20354619, 1.01773091, 0.50886546]\n",
    "np.exp(-np.dot(m, [1,1,18,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(1- 1/(1+ np.exp(-2/3)))*15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(1- 1/(1+ np.exp(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to calculate the gradient and update our model coefficients, we can not perform gradient descent by iterating through this method until our Log Loss converges towards a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code performs the Gradient Descent iterations \n",
    "def GradientDescent_wReg(trainRDD, wInit, nSteps = 20, learningRate = 0.1,\n",
    "                         regType = None, regParam = 0.1, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of regularized gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each update\n",
    "    model = wInit\n",
    "    for idx in range(nSteps):  \n",
    "        # update the model\n",
    "        model = GDUpdate_wReg(trainRDD, model, learningRate, regType, regParam)\n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(LogLoss(trainRDD, model))\n",
    "        #test_history.append(LogLoss(testRDD, model))\n",
    "        model_history.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {LogLoss(trainRDD, model)}\")\n",
    "            print(f\"test loss: {LogLoss(testRDD, model)}\")\n",
    "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
    "    return train_history, test_history, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 50 iterations\n",
    "wInit = BASELINE\n",
    "#trainRDD, testRDD = toy_RDD.randomSplit([0.8,0.2], seed = 5)\n",
    "start = time.time()\n",
    "ridge_results = GradientDescent_wReg(toy_RDD, wInit, nSteps = 1000, learningRate = 0.1, \n",
    "                                     regType=None, regParam = 5.0, verbose = True )\n",
    "print(f\"\\n... trained {len(ridge_results[2])} iterations in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toy_df_label = toy_df.select('label')\n",
    "#toy_df_features = toy_df.select('I1','I2','I3','I4','I5','I6','I7','I8','I9','I10','I11','I12','I13')\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['I1','I2','I3','I4','I5','I6','I7','I8','I9','I10','I11','I12','I13'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "toy_df_transformed = assembler.transform(toy_df)\n",
    "toy_df_transformed = toy_df_transformed.select('features','label')\n",
    "pd.DataFrame(toy_df_transformed.take(30), columns=toy_df_transformed.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"page_num\", \"hour\", \"id\"],\n",
    "    outputCol=\"features\")\n",
    "toy_df_transformed = assembler.transform(dataset)\n",
    "toy_df_transformed = toy_df_transformed.select('features','label')\n",
    "pd.DataFrame(toy_df_transformed.take(30), columns=toy_df_transformed.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=50, regParam=0, standardization=False, fitIntercept=True)\n",
    "lrModel = lr.fit(toy_df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = lrModel.transform(toy_df_transformed)\n",
    "pd.DataFrame(predictions_df.take(30), columns=predictions_df.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot error curves - RUN THIS CELL AS IS\n",
    "def plotErrorCurves(trainLoss, testLoss, title = None):\n",
    "    \"\"\"\n",
    "    Helper function for plotting.\n",
    "    Args: trainLoss (list of MSE) , testLoss (list of MSE)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize = (16,8))\n",
    "    x = list(range(len(trainLoss)))[1:]\n",
    "    ax.plot(x, trainLoss[1:], 'k--', label='Training Loss')\n",
    "    ax.plot(x, testLoss[1:], 'r--', label='Test Loss')\n",
    "    ax.legend(loc='upper right', fontsize='x-large')\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - save and display ridge results (RUN THIS CELL AS IS)\n",
    "trainLoss, testLoss, models = ridge_results\n",
    "plotErrorCurves(trainLoss, testLoss, title = 'Ridge Regression Error Curves' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla \\mathbf{w}=\\sum_{i} \\left(\\frac{1}{1+\\exp \\left(- \\mathbf{\\theta}^{T} \\mathbf{\\bar{x}}_{i}\\right)}-y_i\\right) \\cdot \\mathbf{x}_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Application of Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
