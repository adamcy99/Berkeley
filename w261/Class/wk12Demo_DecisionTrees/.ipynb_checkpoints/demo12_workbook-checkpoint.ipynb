{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wk12 Demo - Decision Trees\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2019`__\n",
    "\n",
    "This week we'll be looking at Decision Trees\n",
    "\n",
    "In class today we'll start out by reviewing the Decision Tree algorithm. We'll look at Regression and Classification trees, learning, pruning, and evluation. We'll extend our discussion to Ensemble methods, including Random Forests, Bagging, and Boosting.  \n",
    "\n",
    "By the end of today's demo you should be able to:  \n",
    "* ... __describe__ Decision Tree CART algorithm  \n",
    "* ... __identify__ Assumptions/constraints for learning DTs\n",
    "* ... __explain__ The difference between regression trees and classification trees\n",
    "* ... __explain__ The difference between bagging and boosting\n",
    "* ... __describe__ The PLANET method for distributing DT learning\n",
    "\n",
    "\n",
    "__`Additional Resources:`__    \n",
    "Chapter 9.2 ESL (OR ISL Chapter 8) - Tree-Based Methods    \n",
    "https://statweb.stanford.edu/~jhf/ftp/trebst.pdf - Greedy Function Approximation - a Gradient Boosting Machine  \n",
    "https://statweb.stanford.edu/~jhf/ftp/stobst.pdf - Stochastic Gradient Boosting  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Decision Tree Review \n",
    "*Based on ESL Chapter 9.2 - Tree Based Mathods*\n",
    "\n",
    "## Benefits\n",
    "* One of the most popular approaches to ML in  practice\n",
    "* Can handle numeric, categorical, and nominal  features\n",
    "* No preprocessing required, no standardization/scaling\n",
    "* Handles Missing values naturally\n",
    "* NAs do not affect performance metrics\n",
    "* Interaction features\n",
    "* Highly Scalable\n",
    "* Variable selection\n",
    "* Excellent performance on a variety of problems\n",
    "* Off the shelf with very few hyperparameters\n",
    "\n",
    "\n",
    "## Approach\n",
    "* A decision tree represents a hierarchical  segmentation of the data\n",
    "* The original segment is called the root node and is the entire data set\n",
    "* The root node is partitioned into two or more segments by applying a series of simple rules  over an input variables\n",
    "* For example, `risk == low`, vs `risk == not low`\n",
    "* Each rule assigns the observations to a segment based on its  input value\n",
    "* Each resulting segment can be further  partitioned into sub-segments, and so  on\n",
    "* For example `risk == low` can be partitioned into  `income == low` and `income == not low`\n",
    "* The segments are also called nodes,  and the final segments are called leaf  nodes or leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig9.2-ESL-tree-diagrams.png\">\n",
    "\n",
    "*Partitions and CART. Left panel shows a partition of a\n",
    "two-dimensional feature space by recursive binary splitting, as used in CART,\n",
    "applied to some fake data. Middle panel shows the tree corresponding\n",
    "to the partition in the left panel, and a perspective plot of the\n",
    "prediction surface appears in the right panel.* __Based on FIGURE 9.2 Elements of Statistical Learning.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees\n",
    "### How do we grow a regression tree?\n",
    "\n",
    "Our data consists of $N$ observations with $p$ features. Suppose we partition the data into $M$ regions $R_1, R_2,...,R_M$ , and model the response as a constant $c_m$ in each region.\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{m=1}^{M}c_mI(x \\in R_m) \\tag{9.10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sum of squares criterion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum(y_i - f(x_i))^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the best $\\hat c_m$ is just the average of $y_i$ in region $R_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat c_m = avg(y_i|x_i \\in R_m) \\tag{9.11}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DISCUSSION__ \n",
    "\n",
    "* Can we find the best binary partition in terms of minimum sum of squares? \n",
    "* What is the big $O$ complexity of this problem?\n",
    "* What is another criterion often used for regression tree partitioining?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__   \n",
    "\n",
    "__Greedy algorithm__   \n",
    "Starting with all of the data, consider a splitting variable $j$ and a split point $s$, we define the pair of half planes:\n",
    "\n",
    "$$\n",
    "R_1(j,s) = \\{X|X_j \\leq s\\} \\text{ and } R_2(j,s) = \\{X|X_j \\gt s\\} \\tag{9.12}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek the splitting variable $j$ and split point $s$ that solve\n",
    "\n",
    "$$\n",
    "\\min_{j,s}[\\min_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i - c_1)^2 + \\min_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i - c_2)^2] \\tag{9.13}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any choice $j$ and $s$, the inner minimization is solved by\n",
    "\n",
    "$$\n",
    "\\hat c_1 = avg(y_i|x_i \\in R_1(j,s)) \\text{ and } \\hat c_2 = avg(y_i|x_i \\in R_2(j,s)) \\tag{9.14}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each splitting variable, the determination of the split point $s$ can be done very quickly and hence by scanning through all of the inputs, determination of the best pair $(j, s)$ is feasible.\n",
    "\n",
    "Having found the best split, we partition the data into the two resulting regions and repeat the splitting process on each of the two regions. Then this process is repeated on all of the resulting regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DISCUSSION__ \n",
    "\n",
    "Consider three types of variables: continuous, ordered (ex. ratings), and categorical\n",
    "* How many split points will we have? \n",
    "* Notice that in equation 9.12, we split our data based on whether it is smaller or larger than our split point $s$. How can we find split points for categorical variables (ie, variables which are not ordered)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"types-of-variables.png\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brieman's theorem\n",
    "\n",
    "For unordered domains, there are $2^X$ possible splits, where X is the number of categories.   \n",
    "\n",
    "Breiman presents an algorithm for finding the best split predicate for a categorical attribute without evaluating\n",
    "all possible subsets of $X$, based on the observation that the optimal split predicate is a subsequence in the list of values for $X_i$ sorted by the average $Y$ value.\n",
    "\n",
    "<!-- <img src=\"brieman.png\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1\n",
    "Run the code cells below, and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE DATASET: Run this cell as is\n",
    "import pandas as pd\n",
    "\n",
    "x = [\"c\",\"b\",\"b\",\"c\",\"a\",\"b\",\"a\"]\n",
    "y = [0.8,0.9,1.4,0.6,3.2,2.5,3.0]\n",
    "    \n",
    "\n",
    "df = pd.DataFrame([x,y]).transpose()\n",
    "df.columns = ['x', 'y']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET MEAN VALUES OF Y : Run this cell as is\n",
    "df[\"y\"] = pd.to_numeric(df[\"y\"])\n",
    "mean_y = df.groupby('x').mean().reset_index().sort_values(by=['y'])\n",
    "mean_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__QUESTIONS__\n",
    "* How many possible split points are there to start with?\n",
    "* How many possible split points are there using Brieman's method?\n",
    "* List the splits:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DISCUSSION__ \n",
    "\n",
    "* How large should we grow the tree? What are the tradeoffs?\n",
    "\n",
    "__Cost-Complexity Pruning__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preferred strategy is to grow a large tree $T_0$, stopping the splitting\n",
    "process only when some minimum node size (say 5) is reached. Then this\n",
    "large tree is pruned using cost-complexity pruning, which we now describe.\n",
    "We define a subtree $T ⊂ T_0$ to be any tree that can be obtained by\n",
    "pruning $T_0$, that is, collapsing any number of its internal (non-terminal)\n",
    "nodes. We index terminal nodes by $m$, with node $m$ representing region\n",
    "$R_m$. Let $|T|$ denote the number of terminal nodes in $T$. Letting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_m = \\#\\{x_i \\in R_m\\},\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat c_m = \\frac{1}{N_m} \\sum_{x_i \\in R_m} y_i, \\tag{9.15}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q_m(T) = \\frac{1}{N_m}\\sum_{x_i \\in R_m} (y_i - \\hat c_m)^2),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define the *cost complexity criterion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C_\\alpha (T) = \\sum_{m=1}^{|T|} N_mQ_m(T) + \\alpha|T|. \\tag{9.16}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to find, for each $α$, the subtree $Tα ⊆ T_0$ to minimize $C_α(T)$.\n",
    "The tuning parameter $α ≥ 0$ governs the tradeoff between tree size and its\n",
    "goodness of fit to the data. Large values of $α$ result in smaller trees $T_α$, and\n",
    "conversely for smaller values of $α$. As the notation suggests, with $α = 0$ the\n",
    "solution is the full tree $T_0$.\n",
    "\n",
    "Estimation of α is achieved by five- or tenfold cross-validation: we choose\n",
    "the value $\\hat α$ to minimize the cross-validated sum of squares. Our final tree\n",
    "is $T \\hat α$. See Breiman et al. (1984) or Ripley (1996) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Trees\n",
    "\n",
    "__DISCUSSION__\n",
    "* What modifications do we need to make for classification trees?\n",
    "\n",
    "__SOLUTION__\n",
    "* If the target is a classification outcome taking values $1, 2, . . .,K$, the only\n",
    "changes needed in the tree algorithm pertain to the criteria for splitting\n",
    "nodes and pruning the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"purity-functions.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__FIGURE 9.3.__ *Node impurity measures for two-class classification, as a function\n",
    "of the proportion p in class 2. Cross-entropy has been scaled to pass through\n",
    "(0.5, 0.5).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a node $m$, representing a region $R_m$ with $N_m$ observations, let"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"class-proportion.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the proportion of class $k$ observations in node $m$. We classify the observations\n",
    "in node $m$ to class $k(m) = arg max_k \\hat p_{mk}$, the majority class in\n",
    "node $m$. Different measures $Q_m(T)$ of node impurity include the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"purity-equations.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three are similar, but crossentropy and the Gini index are differentiable, and hence more amenable to numerical optimization. \n",
    "\n",
    "In addition, cross-entropy and the Gini index are more sensitive to changes\n",
    "in the node probabilities than the misclassification rate. For example, in\n",
    "a two-class problem with 400 observations in each class (denote this by\n",
    "(400, 400)), suppose one split created nodes (300, 100) and (100, 300), while\n",
    "the other created nodes (200, 400) and (200, 0). Both splits produce a misclassification\n",
    "rate of 0.25, but the second split produces a pure node and is\n",
    "probably preferable. Both the Gini index and cross-entropy are lower for the\n",
    "second split. For this reason, either the Gini index or cross-entropy should\n",
    "be used when growing the tree. To guide cost-complexity pruning, any of\n",
    "the three measures can be used, but typically it is the misclassification rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Why binary splits?__\n",
    "Rather than splitting each node into just two groups at each stage (as\n",
    "above), we might consider multiway splits into more than two groups. While\n",
    "this can sometimes be useful, it is not a good general strategy. The problem\n",
    "is that multiway splits fragment the data too quickly, leaving insufficient\n",
    "data at the next level down. Hence we would want to use such splits only\n",
    "when needed. Since multiway splits can be achieved by a series of binary\n",
    "splits, the latter are preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Distributed Tree Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLANET: Massively Parallel Learning of Tree Ensembles with MapReduce\n",
    "https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36296.pdf   \n",
    "\n",
    "\n",
    "The greedy tree induction algorithm we have described\n",
    "is simple and works well in practice. However, it does not\n",
    "scale well to large training datasets. FindBestSplit requires\n",
    "a full scan of the node’s input data, which can be large at\n",
    "higher levels of the tree. Large inputs that do not fit in main\n",
    "memory become a bottleneck because of the cost of scanning\n",
    "data from secondary storage. Even at lower levels of the tree\n",
    "where a node’s input dataset D is typically much smaller\n",
    "than D, loading D into memory still requires reading and\n",
    "writing partitions of D to secondary storage multiple times.\n",
    "\n",
    "PLANET uses MapReduce to distribute and scale tree induction to very large datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Ensembles and gradient boosting\n",
    "\n",
    "The basic idea with ensembles is that several independent models can be combined to make a better model.  One example of this comes from the Netflix prize where several of the top competitors for the million dollar prize joined forced and averaged their models to produce a superior model.  There were a handful of competitors and that was enough improvement to put them into a tie for the best results.  \n",
    "\n",
    "Generally, ensembles include many more than a handful of fully trained models.  The package guidance for the R package gbm (gradient boosting machine) suggests using 3000 models, for example.  How can that many different models be generated?  All of the models need to be solving more or less the same problem.  You can't do them by hand.  You need a systematic method for generating these models.  We'll look quickly at two different methods.  \n",
    "\n",
    "\n",
    "### Bagging\n",
    "First we'll look at Bootstrap Aggregation (called bagging).  That was invented by late Professor Leo Breiman, the famous Berkeley statistician.  Professor Breiman invented bagging to deal with the well known high-variance of binary decision trees.  Here's how it works.  \n",
    "\n",
    "Generate a multitude of different training sets for the same problem.  Train a binary decision tree for each training set and average the results.  To generate multiple training sets take a random sample of the data.  The nominal formula for generating random training sets is to take a sample whose size is 50% of the original data set and extract the data from the original by sample with replacement.  Here's are some simple example to illustrate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Data generator.  \n",
    "import numpy as np \n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def EnsembleDataGen(npts, stdDev):\n",
    "    #Define data set:\n",
    "    #Starter is X sampled regularly in [-10, 10], Y = X + noise\n",
    "    #Try swapping Y = X + noise for Y = np.sin(X) + noise\n",
    "    X = np.linspace(-10.0, 10.0, npts)\n",
    "    Y = X + np.random.normal(0.0, stdDev, npts)\n",
    "    #Y = np.sin(X) + np.random.normal(0.0, stdDev, npts)\n",
    "    return X.reshape([-1,1]), Y.reshape([-1,1])\n",
    "\n",
    "X,Y = EnsembleDataGen(100, 1.0)\n",
    "plt.scatter(X,Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building trees for bagging\n",
    "The core idea for bagging is to build high variance (complex) trees and then overcome the high-variance by average.  With binary decision trees the depth controls complexity (and variance).  So the trees for bagging are deeper than you might train if you were only building one tree and were trying to do the best trade off between bias and variance for a single tree.  One of the benefits of ensemble methods is that they don't require quite as much fussing with regularization parameters as single trees.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Take random samples with replacement, build trees for each one and average.  \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#define weighted sum function to accumulate - (function currying)\n",
    "def wt_sum_fcn(f1, f2, wt):\n",
    "    def wsf(x):\n",
    "        return (f1(x) + wt * f2(x))\n",
    "    return wsf\n",
    "\n",
    "def Bagging(nTrees, nDepth, bagFrac, X, Y):\n",
    "    \"\"\"\n",
    "    nTrees - number of trees in ensemble\n",
    "    nDepts - max depth of trees\n",
    "    bagFrac - fractional size of bags relative to full data set\n",
    "    X, Y - features, labels\n",
    "    \n",
    "    Return: Prediction function that is average of prediction functions of trees in ensemble\n",
    "    \"\"\"\n",
    "    nDataPts = len(X)\n",
    "    wt = float(1.0 / nTrees)\n",
    "    nSamp = int(bagFrac * nDataPts)\n",
    "        \n",
    "    #Define function T to accumulate average prediction functions from trained trees.  \n",
    "    #initialize T to fcn mapping all x to zero to start \n",
    "    T = lambda x: 0.0\n",
    "    \n",
    "    #loop to generate individual trees in ensemble\n",
    "    for i in range(nTrees):\n",
    "        \n",
    "        #take a random sample from the data\n",
    "        sampIdx = np.random.choice(nDataPts, nSamp)\n",
    "        xTrain = X[sampIdx]\n",
    "        yTrain = Y[sampIdx]\n",
    "        \n",
    "        #build a tree on the sampled data\n",
    "        tree = DecisionTreeRegressor(max_depth=nDepth)\n",
    "        tree.fit(xTrain, yTrain)\n",
    "        \n",
    "        #Add the new tree with a weight\n",
    "        T = wt_sum_fcn(T, tree.predict, wt)\n",
    "    return T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFPBFROKEAr6ISJz4/yGPNLKjiBhDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2e124376d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nTrees = 3  #try changing the number of trees being built\n",
    "nDepth = 6   #fairly deep for 100 data points\n",
    "bagFrac = 0.5   #Bag fraction - how many points in each of the random subsamples.  \n",
    "\n",
    "bag = Bagging(nTrees, nDepth, bagFrac, X, Y)\n",
    "\n",
    "result = bag(X)\n",
    "\n",
    "plt.plot(X, result, 'r')\n",
    "plt.scatter(X,Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some things to try\n",
    "1.  Change the number of trees in the ensemble through a range of values 1, 3, 5, 10.  Notice how the prediction smooths out.  \n",
    "2.  Change the trees to depth 1 trees.  What happens as you put more and more trees into the ensemble?  This is a good example showing that no amount of averaging will overcome a bias error.  This is why it's important to grow deep trees for bagging.  \n",
    "2.  In the code generator there's a suggestion to change the dependence of Y on X into a sinusoid.  Make that change and try some values for tree depth, number of trees in the ensemble to see what effect it has.  Also change the number of points in the data set and see what's required to get a relatively smooth fit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "Gradient boosting operates on a different principal from bagging.  The principal is easiest to explain for a regression problem like the one you just saw for bagging.  The idea with gradient boosting is that you fit a tree to the problem, then generate predicitons with that tree and subtract a small amount of the tree's prediction from the original regression labels.  Then the next tree gets trained on the leftovers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GradientBoosting(nTrees, nDepth, gamma, bagFrac, X, Y):\n",
    "    nDataPts = len(X)\n",
    "    nSamp = int(bagFrac * nDataPts)\n",
    "    \n",
    "    #Define function T to accumulate average prediction functions from trained trees.  \n",
    "    #initialize T to fcn mapping all x to zero to start \n",
    "    T = lambda x: 0.0\n",
    "    \n",
    "    #loop to generate individual trees in ensemble\n",
    "    for i in range(nTrees):\n",
    "        \n",
    "        #take a random sample from the data\n",
    "        sampIdx = np.random.choice(nDataPts, nSamp)\n",
    "        \n",
    "        xTrain = X[sampIdx]\n",
    "        \n",
    "        #estimate the regression values with the current trees.  \n",
    "        yEst = T(xTrain)\n",
    "        \n",
    "        #subtract the estimate based on current ensemble from the labels\n",
    "        yTrain = Y[sampIdx] - np.array(yEst).reshape([-1,1])\n",
    "        \n",
    "        #build a tree on the sampled data using residuals for labels\n",
    "        tree = DecisionTreeRegressor(max_depth=nDepth)\n",
    "        tree.fit(xTrain, yTrain)\n",
    "                \n",
    "        #add the new tree with a learning rate parameter (gamma)\n",
    "        T = wt_sum_fcn(T, tree.predict, gamma)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xt8VNW5//HPkxAggBIQVAhFaFVa\nbwWMN9BWsQWUUw22VtRzar0UPa2eVns4xWqt9fKDalv7szcvrW31KGC9UOulgIL3KwgoFFAUUAIV\nqAQvRAiwzh8zEyYze8/smew9mcx8369XXiR7dmavmQzPrFnrWc8y5xwiIlL6Ktq7ASIiUhgK+CIi\nZUIBX0SkTCjgi4iUCQV8EZEyoYAvIlImFPBFRMqEAr6ISJlQwBcRKROd2rsByfr06eMGDRrU3s0Q\nEelQFixYsMk51zfbeUUV8AcNGsT8+fPbuxkiIh2Kma0Jcp6GdEREyoQCvohImVDAFxEpEwr4IiJl\nQgFfRKRMKOCLiJQJBXwRkTJRVHn4IiLlYObCBm6ctYJ1jU30r6lm0pgh1A+rjfy6CvgiIgU0c2ED\nlz/wOk3NOwFoaGzi8gdeB4g86Cvgi4hExKsnf+OsFS3BPqGpeSc3zlqhgC8i0hH59eRTg33Cusam\nyNukSVsRkQj49eQrzTzP719THXmbFPBFRCLg12Pf6RzVVZWtjlVXVTJpzJDI26SALyISAb8ee21N\nNVNOO5Tammos6Wdl6YiIdFCTxgxJG7NP9OTrh9UWJMCnUsAXEYlAIqC3R769HwV8EZGItFdP3k8o\nY/hmdoeZbTCzJUnHepvZHDN7M/5vrzCuJSIi+Qlr0vZPwNiUY5OBJ5xzBwBPxH8WESk5Mxc2MHLq\nXAZPfoSRU+cyc2FDezfJkznnwrkjs0HAw865Q+I/rwCOd86tN7N+wJPOuYx5R3V1dU572opIR5K6\nwArAAEcsA6cQ4/ZmtsA5V5ftvCjTMvdxzq0HiP+7d4TXEhFpF14LrBLd6MTq2mLp8bd7Hr6ZTTSz\n+WY2f+PGje3dHBGRnGQriZCok1MMogz478WHcoj/u8HrJOfcbc65OudcXd++fSNsjohI+IKURChE\nnZwgogz4DwHnxL8/B/hrhNcSEWkXk8YMSSuVkKoQdXKCCCUP38ymAccDfcxsLfBjYCpwr5mdD7wD\nnB7GtUREiknyAquGxqaWCduEQtXJCSKUgO+cO9PnphPDuH8RkWLjt2tVe+1mFYRW2oqI5CjbrlXF\nEuBTKeCLiASU6L03eEzCFmrXqrZQwBcRSeE1LANk3LEKMmfjFMNQjwK+iEgSv+GarlUVGYM9+Gfj\ntOfG5ckU8EWk5OXSu/bbmjBbsM+UjdOeG5cnU8AXkZKWa+86n0VS2Wrm+N1noRdktXtpBRGRKGXq\nXXvxG5apqa7y3Iv2l2cM5bnJozL21P3us9ALshTwRaSk5dq79lo5W11VydWnHJz3XrR+91noBVka\n0hGRkta/ptozjdKvd51ta8J8xtyLZbvD0Orhh0H18EWkLYKmU1ZXVQbunXcExVAPX0SkYBKTsw2N\nTThaT87mOxRTajSkIyIlIdPkbLZJ1XKhHr6IlIRiSX0sZurhi0hJyHVyFoItyCqGkghhUQ9fREpC\nrqmPfmP+yfvPBjmnI1HAF5EOYebCBkZOncvgyY8wcurctKBbP6w2p8nZIAuycl20Vew0pCMiRS9o\neYRcatEHGfMvtXkB9fBFpOhF0dMOUu6gWEoihEUBX0SKXj497WxDQF5j/kbs00Pi/GIpiRAWBXwR\nKXq59rSDTLYmj/kDrTYfL9VFWyqtICJFL3UMHzKXRxg5da5nimZtTTXPTR7V5vOLTdDSCpq0FZGi\nl2vxsVyHgEptctaPAr6IdAi5ZODkuggrn0VbHZHG8EWk5OQ62Vpqk7N+1MMXkZKT6xBQsdSrj5om\nbUWkXbSlRk0p1bcJgyZtRaRo5bqxeFi/W+4U8EWk4LKtnM3Ue8/0uwr4mSngi0jB+aU7JnrrmXrv\n5ZJCGQVl6YhIXrKVLsjEL92x0ixrzZxSq29TSAr4IpKzttaJ90uD3OmTRJLcey+XFMooKOCLSM7a\nWr3Sr3Z9rU8v3UHLp4hc697LbhrDF5GchTGO7rdyNrVmTkLqeH4oAX7LFjj6aFi3ru33lY/zzoOb\nbirY5RTwRSRNtjz3qEoRJC+A8rr/MLJxkh/bfy/+K99Zvhwuugi6ds37PvM2YkRBL6eALyKtBMlz\nnzRmiGf1ynzH0b3eYC6dsQivEf3UTxG5LMJKfmxddmzn9Gfu47nBw9g48cqyGBLSGL6ItBJkfD7M\ncXS/CeCablWe5yd/ish18jj5sZ22ZC57f7yZXx/1tQ67R22u1MMXkVaCjs+HNY7u9wbTpVMF1VWV\nGT9F5LoIK/EYKnbt5MKX7mdRvwN4YeBhWJnk8Efewzez1Wb2upktMjMVyhEpcvnkubclJ9/vDWZL\nU3PWTxG5Th4nHsNJK55nUON6fnfU6WBWNjn8herhn+Cc21Sga4lIG+Q6Pp9pzB+yV6DMNAHc6lPE\n9u1w660wc3co+dErq/jwkx1pv7tH107w45fTjv9h/Qc8sXwDY5c+xVu9BzD7wKPLKodfQzoi0kqu\npYL9hlW+N2OR7z6xyfcV6A3GObjgArjrrlbXOS/TA5mbfuiz8a8dFZVcNu4y+vfqXlaVNgsR8B0w\n28wccKtz7rYCXFNE2iCX8flMufepWTZe4+uB3mCuuioW7K+9Fq68stV95lMquRNwc6BHV1oir4dv\nZv2dc+vMbG9gDnCJc+7ppNsnAhMBBg4cePiaNWsibY+I5M8ruPrlzPsxYNXUcbEfmppg27bMvzBj\nRixP/vzz4fbbwSz/B1CigtbDL+gGKGZ2NfCRc+5nXrdrAxSR4pMI8g2NTa2GaCA29PLVw2u5f0GD\n5+pYL7U11Tw3eRQ8+SSMGwdbt2b/pTFj4G9/gyrvVM1yVxQboJhZd6DCOfdh/PvRwDVRXlNEwpM6\nIes1RDNv+UamnHZooJ5+y9j81q2xHnu/fnDxxZkb0a0bnHWWgn0Ioh7D3wd40GIfwToB9zjn/h7x\nNUUkJF4TsqnWNTa1jPmnvkEALZ8KamuqOeGzfblx1go2XfgrLnj7bZ65/S8cd8HXon0Q0iLSgO+c\nexv4fJTXEJHs8t0DNkgxtOQc9kwTsIk3gyFr/sG58x/if4eexPVrejAlXgFToqe0TJES15Y8eb8c\n+YRW6ZMffwzTplG/dSv13YBu8ZOeWQLPwKo5b3BGUzNnL3qM93r0Zurx52prwgJTwBcpcX558lc/\ntJRtO3blXCQteYim5Q1i06bYBOzL6YudEi6N//tJp85cOP4KPuoSe0fQ1oSFo4AvUuL8AmpjU3Pa\nsdQed6Ac+VWrYOxYeOcd+MtfYNQoz+uN/eXTrN/yCdsrq2jqvLsUcbmUNSgGCvgiJS7bsEyywe83\ncOV9v+elOxyD9urOPnt2oR6oTz5pXsovLVwIzc0wZw4ce6zvfV80/ohQSypL7lQeWaTE+e0B28uj\n/PBFL97HsasXUfXxh6x/9z3eX78JPvgg89ewYfDssxmDPYRbUlnyox6+SIlKzszpWV1F16oKGrc2\ntwzLQOvtBPf85CNOWfY09x9yIj8cG8uNb1kkFZLQtiaUvCjgi5Sg1MycxqZmqqsquemMoWkBN7Fg\navzSeVTv2Mbdw05quU0TqqVFAV+kBGXbtSqtHs7fl3P2wsdY1O9Alu7zmZbf0YRqadEYvkgJ8uuZ\nJ1IvU7cE/KZ7lwP/9Q53D93du9eEaulRwBcpQX4980ozz57/fvf+L8099mTB0aM1oVrCNKQjUoIS\nC6Z2NTVxzJrXqHQ76VxZwfadu9LOrdq5gy8ueYqq73ybuVed3A6tlUJRwBcpQYme+YYf/IiJc/6Y\n9fydVgEXXhh1s6SdKeCLlJBWRdJ6dmXW8nkwYgTcHNvfad7yDfx63kq27djd0+/SqYILTxnO6IMO\naq9mS4Eo4IuUiNRUzL3/sYge767m1XMvZvjhhwNwwuGw5aD0ypmjNVZfFhTwRUpEairm+KXz+KRT\nZ35gBzIn6TwtfipfCvgiEcu3Fn2u959cL6dqZzNfWfY0sw84mpWfKBlPYhTwRSKUqRZ9GEHfa4cp\ngOPfXkCvTz7kgYNP0OIpaaGALxKhTCtecw34Xp8U/LYgHL9kLhu71TD/wCO4TounJE4BXyRCfite\nc61R4/dJYfu27XRyrbcW33Pbx5z41sv89ahTuO5r6bVzpHwp4ItEyK8Wfa7DLF49+fOeuofLnr2b\nSpe+mArg6zf/EBTsJYkCvkiEvLYIzKdGTeongvql85j0zF3M2f9Ilg74LM07d/fyqyqNL3zx8wwf\nPrxtjZeSo4AvEqFAWwQGkPxJ4Yh3l/DTx/4/zw88jOvOuYZLxx2Sdv/D1bMXD+ZSxv/aU11dnZs/\nf357N0Ok6CTG8PtuWMvMu77P5uo9OevcX3D52SM0Ri+Y2QLnXF2289TDF+kA6ofVUrHtEz532sWY\nc1x+7v9TsJecKeCLhCSsBVZ+93PKnT+H9W/Bww9z77hxETwCKXUK+CIhyGeBlVdgBzzvp3bWQxzx\nu9/BpEmgYC950hi+SAhGTp3rmX7ptwm41wrZwVvf5ysrnqFpe+v0y0q3i0tevJfuwz8PTz0FVVXh\nPwDp0DSGL1JAuS6w8sqr/59Zt3DSG897388efeg+fbqCvbSJAr5ICHJdYJX6RrDvB5v48psvcvsR\n9dx07Nlp5/fZqydPDxwYTmOlbKmMnkgIJo0ZQnVVZatjmRZYpb4RnLn471Q4x4MjxuO692Br5+qW\nL9e9B5ed3HpzkpkLGxg5dS6DJz/CyKlzmbmwIdwHJCVJAV8kBPXDaply2qHU1lQH2gQ8+Q2iamcz\nZy3+O0/vfwQTzzkx6/0kxv8bGptw7J7YVdCXbDSkIxKSXDYWSV6BO/z5p+j7cSOdv3txy/HEv4lM\nnktnLMpYITPfCpxSXpSlI9LejjsO1q2DN9+Eit0fur0yeaqrKj3LIQMYsGqqUjbLkbJ0RMLwwQew\nZInvzU+/sZG7X3qHTR9uo88eXTj7qIF84cC+gc8//9OdOfLZZ+HGG1sFe/CvpV9pxk6Pjpo2OpFs\nFPCl7CUvgOpZXYUZNG5tpn9NNfc+8QtqH3/E93e/EP9qcUvma6WdD+zoWk2nc89NO9cvpXOnc2k9\n/XwqcEr5UcCXspY6bNLY1Nxy26ZNW+j11OM0fGkcq8efxX3z17Lp420YkGkgtE/3Ltw0YWja8Uun\nL2LTx9vSju+o/RTT9tor7bhfqmdt0lh+VPvkSmlSwJeSk0tNG78tAgGOWfMa3Zq3MWmfY5i7vjdN\ne/cMdH0Dbho9Or1dc5s93yjM534y1dLPZYJYJCHytEwzG2tmK8xspZlNjvp6Ut5yTVnMtNXgiW+9\nzMdVXXl834N83xS8+I2l53o811RPkWwi7eGbWSXwG+DLwFrgFTN7yDn3jyivK+Ur15RFv2ETnGPU\nyld4dtBQtnXqHPj6mcbS89n9Sj15CVPUPfwjgZXOubedc9uB6cCpEV9TyliuNW28VsgCHLRhFbUf\nbuTx/Y/Mes3EkEy2Hrh67NLeoh7DrwXeTfp5LXBUxNeUMpZrTZvULQgTWTqj3noZgCc/fYTn7yUm\nbmtznDBVj13aU9QB32s+qtW8lZlNBCYCDFRxKGmjsIZNFt1yCYv6HcjGHr3Szs81yAcR1uYpIplE\nHfDXAp9K+nkAsC75BOfcbcBtEFtpG3F7pMSFsmn4P//J0PVv8LPj/j3tJr/69m2Rz+YpIvmIOuC/\nAhxgZoOBBmACcFbE15QyVz+slvoDesLq1fEjm2HJ5uB38PDDADz72WNaHY5qcZNq40ihRBrwnXM7\nzOxiYBZQCdzhnFsa5TVFAPjKV+DJJ/P//f3245sXncKNs9+IfJgl14lmkXxFvvDKOfco8GjU1xFp\nsXp1LNiffz6MHZvffRx6KPVDBlA/fECYLfOU60SzSL600lZKz7RpsX+vvBIGDWo5XKwTo/lMNIvk\nQwFfil7Ogfqee2DkyLRgX6wTo6FMNIsEoIAvRS3nQP3aa7Fyxr/5TavDhZ4YzfVNSvn5Ugja4lCK\nWqZA7emee6CyEk4/vdXhQk6MagtCKVYK+FLU/AJyQ2NT+ubdu3bFAv6YMdA3tglJYrNvvwUeDkLf\nBDznNymRAlHAl6KWKVMlref83HPw7rtwVmypR3JPO5Owe+BKs5RipTF8KbhWO0x17cSEVx7igHeW\n07ky1v/YvnMX3aoqObi2J9OAV9ds9tzSL6HLI5VwaL/Y+H23bnBqrD5fplr3qcIcz1eapRQrBXwp\nqNRJ2PNm/5H/emEG/+zRm+bKqlbn2tvQq3tnvgxsaWpm564MlTc2vxn799JLoUcPIPcedVg9cKVZ\nSrFSwJeCSu51f/uFe/mvF2Yw7bDR/HDsxThLH2FM1K7pTmys3W/LP6/6Nn497ag3AVeapRQrBXwp\nnNmz+frf/gTA3h/9izNfm80DB5/AFWO+4xnsoXWv26vnbOyewE0Nqn497a8eXsv9Cxoi7YErzVKK\nkQK+FMYbb8DJJ/PdnbuD7P0Hn8D/nPw9dlWkb0CSkNzrTu45NzQ2tdpM3Cs/P1NPu26/3uqBS9kx\nl2EyrNDq6urc/Pnz27sZEoGGL/8bvZ5+gi9+63Y29ejlmyaZLNMmI37DO/icL1LKzGyBc64u23lK\ny5TIPXXn36h9/BFurxvPxniwT+yMU1NdRa9uVVjS94Bn7z05bTLTBKsWOol405CORMs59rj6R2zq\n1pPbjhy/+zD+k61evffUtEnfzcd9zhcR9fAlao89xvBVi7l5xAQ+7tKt1U25LlBKncD12nw8yP2I\nlCsFfInOzp0weTJre/dn2tD0uvR+aZBBjtcPq2XKaYdSmyGVUgudRFpTwJdQJWrXDJ78CNeceSW8\n/jrrJ11Bp65dW52XKQ3Sq/fudX79sFqemzyKX54xNND5IuVOY/gSmuRVtJ13NHPu7D/yj30/Q8OX\nv8KUiorAaZC5LlzSQieRYJSWKaFJnmw9Z8Hf+Mnjt/KN03/CW8OP9ZycFZFwBE3LVA9fQpOYJO2+\nbSuXPD+d5wcextODh2OaPBUpChrDl9AkJknPm/9X+mzdwg1fPAfMNHkqUiQU8CU0k8YM4TMfb+Ki\nl+7nsQNHsKj/EE2eihQRDelIm7XUt9+8lemP/BJXUcF1J16gEgciRUYBv4zlutG2330kMnPOXvQY\nR61axFUnX8Kkb41WoBcpMhrSKVNhbbSdqG8/YMt7XP7kH3lmv6Hcecho7d8qUoQU8MtUWBttr2ts\nAueY8tivALj8pEvATGUNRIqQAn6ZCmuj7f411XxtyRMct2YRU7/4Tdb23KfluIgUFwX8MpVrHRs/\nVxyxF1fO+wOv1B7E3cNOAlTWQKRYKeCXqaD1arI5+Y4b2LP5E355+vfBKqitqWbKaYdqwlakCClL\np0zlU38mNavnxh4NjLjnHip+/GPuvvq8rOcrRVOkfamWTjnbvBmOOw6WL8966i5g167Wr5VObhcf\nDtqfPZYvgS5dWt2WnK6ZUF1Vqd6/SARUS6cM5dyjvuoqWLYMvv996Nw5433f+fxqPvxkR6tju8x4\nbsQ47k0J9pA5C0gBX6R9KOCXiNQedSKvHvAOsIsXw29/C9/+NtxwQ9b7/8nkRzw3HjePYxBeFpCI\nhEeTtiUip7x65+CSS6B3b7jmmkD335bdqYIcF5HoKeCXiJx61NOmwTPPwJQp0KtXxvtN7GDV0NiU\n1psPY9cqESkcDemUiP411S2bjwDUNH3A7fdfx6AP34O7UsbY//UvqKuD89Iza2D3XEAiyCeGchy0\n/JytMJp2oRIpPgr4JWLSmCGtxvB/NPf3DF2/ggcOHkV11yoOG9CTQXt1j51cVQWXXQYV6R/wUucC\nUsftE8E+yA5W9cNqFeBFikhkAd/Mrga+BWyMH/qhc+7RqK5X7pJ71J959Vm+umQuvzrmDH7+hf8A\ngqdEes0FpNLEq0jHFHUP/ybn3M8ivobE1Q+rpX7/PfnnwDNZ2XsAvx5xRsttQVMigwRzTbyKdEwa\n0il2y5bBhg3Bz//zn9m7cSPfOfunbOvUOrc+aDBvyHCeJl5FOq6oA/7FZvYNYD7wfefc5oivV1qm\nT4czz8z51+4bMZ4FAw5KOx6kZ546FwDBJ2pFpLi1qbSCmT0O7Otx0xXAi8AmYrHiWqCfcy4tLcTM\nJgITAQYOHHj4mjVr8m5PSXn9dTj6aBg2DK691vOUZ9/cyO3PrGL7jl27D3bpwv6nnMh9C9d7ljWA\n7JkzqoEj0rEELa1QkFo6ZjYIeNg5d0im81RLJxZsfzdzAbfefBE9dm7nlftmc9Lo4Z7nJvLjUyV6\n4qlBGwitvo3eFESKR7vX0jGzfs659fEfxwNLorpWqZi5sIEf3r+Ym6dfR/8PNjLhzCkse2YD2/o2\neAbTTIutvFIiR06dG0p9m5zLOIhIUYhype0NZva6mb0GnABcGuG1SsKNs1bwn3Pv5EtvvcK1J17A\nqwM+l3HbwVzLF4RV3yas7RFFpLAi6+E75/4jqvsuVcOf/zuXvDCDaYeN5q5h41qOr2ts8hxC8Zpg\nzZRF45eBk2uapQqjiXRMqodfLObPZ9sxI1m87/6cPeF6miurWm6qqa5i245dnpkzNdVVmEHj1uas\nY+mZatRD8DIImeYOgqzAFZFwtfsYfrt5+OFA5X7b26aPtrN281a279hF504VfK5xLTv79OXS03/U\nKthXV1ViRtoQSuJturGpmeqqSm46Y2jW8XO/+jZATmPyuX6yEJHiUHoB3ww6FffD2vjhNt56/xN2\nOYOKSnbsghf6HMCu669n0gGfSwvIl85YlPH+cpl4DWMyV4XRRDqm4o6M+Rg3LvZVxOr9hkTeruC5\nr6cH5ETlykzaMn6ez5i8CqOJdDyqh98Ocg2wXrXlU7Wlvo02KxEpDwr47cAvkDpiwyszFza0Ol4/\nrJYppx1Kbfz3ctmIJAhtViJSHpSlEzGvdEpIX/GaLNvq1yhWuWrlrEjHVVSlFYIqtYAfJA3Sb2xe\nKY4iElTQgK8hnQhlWpFaP6yW5yaPShueSdAiJhEJmwJ+hIJMzmrCVEQKRQE/QkGCeSEmTGcubGDk\n1LkMnvyI56SwiJQHBfwIBQnmyRk4RmzsPp9yxX4S8wgNjU04dq+iVdAXKT+lt/CqiARdkRrlIqZs\n8wgiUj4U8CPW3itSVdlSRBI0pFPiNCksIgkK+DnoiJOfWkUrIglaeBWQ1yKqRE362ppqTvhsX+Yt\n31iUK1W1ilaktGmlbcj8Nv3wk+/m4CIiuSrfDVCyCNLb9Ton10lOr0wY9bRFpD2VVcBPHZbx2tnJ\n75yablVs3tqc0/WS3ySCXFtEJEplNWmbKSc92znOkbUmfarkTJgg107VESeJRaR4lVXAz5STngiu\nfuP0W5qaM9akT5WaCZNrPrxWyIpI2Moq4PvlnvesrmoJrpl+N1HhcvXUcdx0xtBW5RD+/eiBGcsj\n5JoPn88nAhGRTMpqDH/SmCGe9enN8N2MJHFOat56phW0icnZS2csapmc9bu2Xz68VsiKSNjKqofv\nV6isMcNkbK7FzPyGYoCciqRphayIhE15+Pjn2Oez61RY95Vptyxl9YhIMu14lYMwyw+ENRQTddlk\nESk/ZTWG7ydoGeMg+tdUe/bw8xmKae9KmyJSWhTw48IKrrlOzoqIFEpJBfxiKF0Q5qcFEZEwlUzA\nL6bSBRqKEZFiVDIBP6qt/IrhU4OISBhKJuBnK5uQT9Aupk8NIiJtVTJpmUHKJuRak0blDUSklJRM\nDz+XsgnJQTtTz1/lDUSklJRMDz/XsgmJnn6mnr/KG4hIKSmZHj54Z8fcOGuF50KoSrOsk7zKqReR\nUtKmHr6ZnW5mS81sl5nVpdx2uZmtNLMVZjambc3MXXJ9+9Ta9dVVlez0qSGUPFyj8gYiUkra2sNf\nApwG3Jp80MwOAiYABwP9gcfN7EDnnH8N4hClZtc4YhuWOGJBe9KYIb49/9ThGuXUi0ipaFPAd84t\nAzBL2//pVGC6c24bsMrMVgJHAi+05XpBeWXXJIJ9csVKDdeISDmJagy/Fngx6ee18WMFESS7RiUQ\nRKTcZA34ZvY4sK/HTVc45/7q92sexzwHzc1sIjARYODAgdmaE0jQipUarhGRcpJ10tY59yXn3CEe\nX37BHmI9+k8l/TwAWOdz/7c55+qcc3V9+/bNrfU+wqxvLyJSKqLKw38ImGBmXcxsMHAA8HJE10qj\n7BoRkXRtGsM3s/HAr4C+wCNmtsg5N8Y5t9TM7gX+AewAvlOoDJ0EDdeIiLTW1iydB4EHfW67Hri+\nLfcvIiLhKZnSCiIikpkCvohImVDAFxEpEwr4IiJlQgFfRKRMKOCLiJQJcz5lgtuDmW0E1oRwV32A\nTSHcT9iKsV1qU3DF2C61KbhibFdYbdrPOZe1VEFRBfywmNl851xd9jMLqxjbpTYFV4ztUpuCK8Z2\nFbpNGtIRESkTCvgiImWiVAP+be3dAB/F2C61KbhibJfaFFwxtqugbSrJMXwREUlXqj18ERFJ0WED\nvpmdbmZLzWyXmdWl3Ha5ma00sxVmNsbn9web2Utm9qaZzTCzzhG0cYaZLYp/rTazRT7nrTaz1+Pn\nzQ+7HSnXutrMGpLadbLPeWPjz99KM5sccZtuNLPlZvaamT1oZjU+50X+PGV73PE9HmbEb3/JzAZF\n0Y6Ua37KzOaZ2bL4a/67Huccb2Zbkv6uVxWgXRn/HhZzc/y5es3MhkfcniFJj3+RmX1gZt9LOacg\nz5OZ3WFmG8xsSdKx3mY2Jx5z5phZL5/fPSd+zptmdk6oDXPOdcgv4HPAEOBJoC7p+EHAYqALMBh4\nC6j0+P17gQnx728B/jPi9v4cuMrnttVAnwI9b1cD/53lnMr48/ZpoHP8+TwowjaNBjrFv/8p8NP2\neJ6CPG7g28At8e8nADMK8DfrBwyPf78H8IZHu44HHi7Eayjo3wM4GXiM2JanRwMvFbBtlcA/ieWn\nF/x5Ar4ADAeWJB27AZgc/34b0X9dAAAEM0lEQVSy1+sc6A28Hf+3V/z7XmG1q8P28J1zy5xzKzxu\nOhWY7pzb5pxbBawEjkw+wcwMGAXcFz/0Z6A+qrbGr/d1YFpU1wjZkcBK59zbzrntwHRiz2sknHOz\nnXM74j++SGxLzPYQ5HGfSuz1ArHXz4nxv29knHPrnXOvxr//EFgGdITdfU4F7nQxLwI1ZtavQNc+\nEXjLORfGQs6cOeeeBt5POZz82vGLOWOAOc65951zm4E5wNiw2tVhA34GtcC7ST+vJf0/x15AY1KQ\n8TonTMcB7znn3vS53QGzzWxBfFP3qF0c/4h9h8/HyiDPYVTOI9Yr9BL18xTkcbecE3/9bCH2eiqI\n+BDSMOAlj5uPMbPFZvaYmR1cgOZk+3u05+toAv4drEI/Twn7OOfWQ+xNHNjb45xIn7M27XgVNTN7\nHNjX46YrnP8m6l69rdRUpCDnBBKwjWeSuXc/0jm3zsz2BuaY2fJ4DyEvmdoE/A64ltjjvZbYUNN5\nqXfh8bttSucK8jyZ2RXEtsS82+duQn2evJrpcSyy106uzKwHcD/wPefcByk3v0ps+OKj+LzMTGJ7\nSUcp29+jXZ6r+HzcKcDlHje3x/OUi0ifs6IO+M65L+Xxa2uBTyX9PABYl3LOJmIfLzvFe2le54TS\nRjPrBJwGHJ7hPtbF/91gZg8SG1rIO5AFfd7M7HbgYY+bgjyHobYpPjn1b8CJLj6Y6XEfoT5PHoI8\n7sQ5a+N/256kf3QPnZlVEQv2dzvnHki9PfkNwDn3qJn91sz6OOciqx0T4O8R+usooJOAV51z76Xe\n0B7PU5L3zKyfc259fGhrg8c5a4nNMyQMIDZPGYpSHNJ5CJgQz6YYTOzd++XkE+IBZR7wtfihcwC/\nTwxt9SVguXNurdeNZtbdzPZIfE9sAnOJ17lhSBlDHe9zrVeAAyyWydSZ2MfjhyJs01jgB8Apzrmt\nPucU4nkK8rgfIvZ6gdjrZ67fG1RY4nMEfwCWOed+4XPOvom5BDM7ktj/7X9F2KYgf4+HgG/Es3WO\nBrYkhjQi5vuJutDPU4rk145fzJkFjDazXvHh1tHxY+GIerY6qi9iwWotsA14D5iVdNsVxLItVgAn\nJR1/FOgf//7TxN4IVgJ/AbpE1M4/ARelHOsPPJrUjsXxr6XEhjiifN7uAl4HXiP2AuyX2qb4zycT\nywZ5qwBtWkls3HJR/OuW1DYV6nnyetzANcTejAC6xl8vK+Ovn08X4LV+LLGP9a8lPUcnAxclXlvA\nxfHnZTGxie8REbfJ8++R0iYDfhN/Ll8nKZsuwnZ1IxbAeyYdK/jzROwNZz3QHI9T5xOb63kCeDP+\nb+/4uXXA75N+97z462slcG6Y7dJKWxGRMlGKQzoiIuJBAV9EpEwo4IuIlAkFfBGRMqGALyJSJhTw\nRUTKhAK+iEiZUMAXESkT/wdAlv8yasv2FQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2e13edd978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nTrees = 20  #try changing the number of trees being built\n",
    "nDepth = 1   #fairly deep for 100 data points\n",
    "gamma = 0.1\n",
    "bagFrac = 0.5   #Bag fraction - how many points in each of the random subsamples.  \n",
    "\n",
    "gbst = GradientBoosting(nTrees, nDepth, gamma, bagFrac, X, Y)\n",
    "\n",
    "result = gbst(X)\n",
    "\n",
    "plt.plot(X, result, 'r')\n",
    "plt.scatter(X,Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments and some things to try\n",
    "You may have noticed that the sampling machinery from bagging was left in the code for gradient boosting.  Friedman's first paper \"Greedy Function Approximation\" did not include sampling the input data.  But sampling and the basic mechanics of functional gradient descent are separate matters and Friedman's second paper \"Stochastic Gradient Boosting\" added that element.  Links to both these papers can be found below.  \n",
    "\n",
    "#### Some things to try\n",
    "- Try running some of the same experiments as you did with bagging. Change the tree depth, number of trees etc.  Also try the sine function for Y(X) and see how gradient boosting does with it.  \n",
    "\n",
    "Here are some things that will highlight an important difference between gradient boosting and methods that are primarily variance reduction techniques.  \n",
    "\n",
    "- Experiment with different tree depths and see how it affects the accuracy of the final model.  With bagging, you saw that using depth 1 trees resulted in a bias error that could not be overcome by adding more trees.  Does that happen with gradient boosting?  \n",
    "\n",
    "Since gradient boosting is constantly changing the labels to emphasize the portions of the X-space where it's making the most mistakes, it will eventually pay so much attention to the edges of the data that it will start putting split points for depth 1 trees at the extreme edges of the data.  That raises the question: \"Why would you ever use trees deeper than 1 with gradient boosting?\"  \n",
    "\n",
    "The reason for adding tree depth with gradient boosting is to cover problems where there is joint dependence on two or more variables and that dependence plays an important role in predicting the labels.  Modeling two-way or dependence requires that pairs of variables both affect some of the splits in a single tree.  That requires more than a single split in the trees.  Start with relatively shallow trees for gradient boosting.  After you've got that dialed in, then try more depth to see if you get an improvement.  \n",
    "\n",
    "I hope you like gradient boosted trees.  It has won more Kaggle competitions than any other algo.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "https://statweb.stanford.edu/~jhf/ftp/trebst.pdf - Greedy Function Approximation - a Gradient Boosting Machine  \n",
    "https://statweb.stanford.edu/~jhf/ftp/stobst.pdf - Stochastic Gradient Boosting  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
