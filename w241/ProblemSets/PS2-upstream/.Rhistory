ate <- mean(treat.group) - mean(cont.group)
return(ate)
}
# Now we run the simulation 10,000 times as directed
distribution.under.sharp.null <- replicate(10000, simulation(d))
# Plot the density distribution and histogram of our simulations
plot(density(distribution.under.sharp.null),
main = "Density under Sharp Null", xlab = "Simulated ATE")
abline(v = exp_ate, col = "red")
hist(distribution.under.sharp.null,
main = "Histogram under Sharp Null", xlab = "Simulated ATE")
abline(v = exp_ate, col = "red")
# The actual estimate of the ATE is stored as exp_ate
prob1c.answ <- sum(distribution.under.sharp.null >= exp_ate)
paste("Answer:", prob1c.answ)
prob1d.answ <- prob1c.answ/10000
paste("Implied one-tailed p-value:", prob1d.answ)
prob1e.answ <- sum(abs(distribution.under.sharp.null) >= exp_ate)
paste("Answer:", prob1e.answ)
prob1f.answ <- prob1e.answ/ 10000
paste("Implied two-tailed p-value:", prob1f.answ)
library(foreign)
d <- read.dta("./data/Titiunik.2010.dta")
head(d)
# First I will group the data by state to show the mean number of bills introduced.
by_state <- d %>% group_by(texas0_arkansas1)
by_state %>% summarise(avg_num_bills = mean(bills_introduced))
# Now I will group by two- or four-year terms to show the mean number of bills introduced.
by_term <- d %>% group_by(term2year)
by_term %>% summarise(avg_num_bills = mean(bills_introduced))
library(data.table)
library(dplyr)
d <- read.csv("./data/Clingingsmith.2009.csv", stringsAsFactors = FALSE)
by_success <- d %>% group_by(success)
by_success %>% summarize(avg_views = mean(views), sample_size = length(views))
# Calculate the average treatment effect of the observed data
exp_ate <- mean(d[d$success == 1,]$views) - mean(d[d$success == 0,]$views)
exp_ate
# create function to run a simulation of random assignment under the sharp null hypothesis.
simulation <- function(data) {
# First we randomly create assignments for treatment and control group.
# The number of 0's and 1's are not always equal but the probability of getting a 0
# is equal to the probability of getting a 1.
treatment <- sample(c(0,1), size = length(data$views), replace = TRUE)
# Next we put the views data into their corresponding groups that we've randomly assigned
treat.group <- data$views[treatment == 1]
cont.group <- data$views[treatment == 0]
# Now we can calculate the estimated ATE for this specific random assignment of
# control/treatment groups.
ate <- mean(treat.group) - mean(cont.group)
return(ate)
}
# Now we run the simulation 10,000 times as directed
distribution.under.sharp.null <- replicate(10000, simulation(d))
# Plot the density distribution and histogram of our simulations
plot(density(distribution.under.sharp.null),
main = "Density under Sharp Null", xlab = "Simulated ATE")
abline(v = exp_ate, col = "red")
hist(distribution.under.sharp.null,
main = "Histogram under Sharp Null", xlab = "Simulated ATE")
abline(v = exp_ate, col = "red")
# The actual estimate of the ATE is stored as exp_ate
prob1c.answ <- sum(distribution.under.sharp.null >= exp_ate)
paste("Answer:", prob1c.answ)
prob1d.answ <- prob1c.answ/10000
paste("Implied one-tailed p-value:", prob1d.answ)
prob1e.answ <- sum(abs(distribution.under.sharp.null) >= exp_ate)
paste("Answer:", prob1e.answ)
prob1f.answ <- prob1e.answ/ 10000
paste("Implied two-tailed p-value:", prob1f.answ)
library(foreign)
d <- read.dta("./data/Titiunik.2010.dta")
head(d)
# First I will group the data by state to show the mean number of bills introduced.
by_state <- d %>% group_by(texas0_arkansas1)
by_state %>% summarise(avg_num_bills = mean(bills_introduced))
# Now I will group by two- or four-year terms to show the mean number of bills introduced.
by_term <- d %>% group_by(term2year)
by_term %>% summarise(avg_num_bills = mean(bills_introduced))
# Now I will group by both state and term length
by_state_term <- d %>% group_by(texas0_arkansas1, term2year)
blocked_dt <- by_state_term %>% summarise(avg_num_bills = mean(bills_introduced))
blocked_dt
# First calculate the ATE for each state.
tx.ate <- blocked_dt[2,3] - blocked_dt[1,3]
ar.ate <- blocked_dt[4,3] - blocked_dt[3,3]
paste("The ATE for the state of Texas is:", tx.ate)
paste("The ATE for the state of Arkansas is:", ar.ate)
# Method using equation 3.6 in Textbook
# First create 4 separete data tables for the control/treatment group of TX and AR.
tx.4year <- by_state_term %>% filter(texas0_arkansas1 == 0, term2year == 0)
tx.2year <- by_state_term %>% filter(texas0_arkansas1 == 0, term2year == 1)
ar.4year <- by_state_term %>% filter(texas0_arkansas1 == 1, term2year == 0)
ar.2year <- by_state_term %>% filter(texas0_arkansas1 == 1, term2year == 1)
# Calculate Var(Y(1)) using equation 3.7 for Texas
tx.4year.m <- length(tx.4year$bills_introduced)
tx.4year.mean <- sum(tx.4year$bills_introduced)/tx.4year.m
tx.var.Y0 <- sum((tx.4year$bills_introduced - tx.4year.mean)^2)/(tx.4year.m - 1)
# Calculate Var(Y(0)) using equation 3.8 for Texas
tx.2year.m <- length(tx.2year$bills_introduced)
tx.2year.mean <- sum(tx.2year$bills_introduced)/tx.2year.m
tx.var.Y1 <- sum((tx.2year$bills_introduced - tx.2year.mean)^2)/(tx.2year.m - 1)
# Calculate SE of the ATE using equation 3.6 for Texas
tx.SE.ate <- sqrt(tx.var.Y0/tx.4year.m + tx.var.Y1/tx.2year.m)
paste("The estimated SE of the ATE for Texas is", tx.SE.ate)
# Calculate Var(Y(1)) using equation 3.7 for Arkansas
ar.4year.m <- length(ar.4year$bills_introduced)
ar.4year.mean <- sum(ar.4year$bills_introduced)/ar.4year.m
ar.var.Y0 <- sum((ar.4year$bills_introduced - ar.4year.mean)^2)/(ar.4year.m - 1)
# Calculate Var(Y(0)) using equation 3.8 for Arkansas
ar.2year.m <- length(ar.2year$bills_introduced)
ar.2year.mean <- sum(ar.2year$bills_introduced)/ar.2year.m
ar.var.Y1 <- sum((ar.2year$bills_introduced - ar.2year.mean)^2)/(ar.2year.m - 1)
# Calculate SE of the ATE using equation 3.6 for Arkansas
ar.SE.ate <- sqrt(ar.var.Y0/ar.4year.m + ar.var.Y1/ar.2year.m)
paste("The estimated SE of the ATE for Arkansas is", ar.SE.ate)
# Method using regression
# First create a data table for only texas samples
tx.data <- by_state_term %>% filter(texas0_arkansas1 == 0)
# Do a linear regression
tx.regression <- summary(lm(tx.data$bills_introduced ~ tx.data$term2year))
# SE given in the regression summary
tx.se <- tx.regression$coefficients[2,2]
paste("The estimated SE of the ATE for Texas is", tx.se)
# First create a data table for only Arkansas samples
ar.data <- by_state_term %>% filter(texas0_arkansas1 == 1)
# Do a linear regression
ar.regression <- summary(lm(ar.data$bills_introduced ~ ar.data$term2year))
# SE given in the regression summary
ar.se <- ar.regression$coefficients[2,2]
paste("The estimated SE of the ATE for Arkansas is", ar.se)
tx.N <- length(tx.data$bills_introduced)
ar.N <- length(ar.data$bills_introduced)
N <- length(by_state_term$bills_introduced)
tx.ate <- as.numeric(tx.ate)
ar.ate <- as.numeric(ar.ate)
Overall.ATE <- (tx.N/N)*tx.ate + (ar.N/N)*ar.ate
Overall.ATE
# I will be using the standard errors obtained from the regression method
sqrt((tx.se^2)*(tx.N/N)^2 + (ar.se^2)*(ar.N/N)^2)
# create function to run a simulation of random assignment under the sharp null hypothesis.
simulation <- function(data) {
# First we randomly create assignments for treatment and control group.
# The number of 0's and 1's are not always equal but the probability of getting a 0
# is equal to the probability of getting a 1.
treatment <- sample(c(0,1), size = length(data$bills_introduced), replace = TRUE)
# Next we put the views data into their corresponding groups that we've randomly assigned
treat.group <- data$bills_introduced[treatment == 1]
cont.group <- data$bills_introduced[treatment == 0]
# Now we can calculate the estimated ATE for this specific random assignment of
# control/treatment groups.
ate <- mean(treat.group) - mean(cont.group)
return(ate)
}
# Now we create a function to find the overall ATEs for our simulations
Overall_ATE_Sim <- function(txdata, ardata) {
tx.sim.ate <- simulation(txdata)
ar.sim.ate <- simulation(ardata)
tx.N <- length(tx.data$bills_introduced)
ar.N <- length(ar.data$bills_introduced)
N <- length(by_state_term$bills_introduced)
OverallATE <- (tx.N/N)*tx.sim.ate + (ar.N/N)*ar.sim.ate
return(OverallATE)
}
# Now we run the simulation 10,000 times as directed
distribution.under.sharp.null <- replicate(10000, Overall_ATE_Sim(tx.data, ar.data))
# Plot the density distribution and histogram of our simulations
plot(density(distribution.under.sharp.null),
main = "Density under Sharp Null", xlab = "Simulated ATE")
abline(v = Overall.ATE, col = "red")
hist(distribution.under.sharp.null,
main = "Histogram under Sharp Null", xlab = "Simulated ATE")
abline(v = Overall.ATE, col = "red")
# Find the implied one-tailed p-value
onetail <- sum(distribution.under.sharp.null <= Overall.ATE)/10000
paste("Implied one-tailed p-value:", onetail)
#Find the implied two-tailed p-value
twotail <- sum(abs(distribution.under.sharp.null) >= abs(Overall.ATE))/10000
paste("Implied two-tailed p-value:", twotail)
layout(matrix(c(1,2,3,4), 2, 2, byrow = T))
hist(tx.2year$bills_introduced, main = "Texas, 2years (Treatment)", xlab = "# of Bills Introduced in 2003")
hist(tx.4year$bills_introduced, main = "Texas, 4years (Control)", xlab = "# of Bills Introduced in 2003")
hist(ar.2year$bills_introduced, main = "Arkansas, 2years (Treatment)", xlab = "# of Bills Introduced in 2003")
hist(ar.4year$bills_introduced, main = "Arkansas, 4years (Control)", xlab = "# of Bills Introduced in 2003")
## load data
d <- read.csv("./data/ggChapter3.csv", stringsAsFactors = FALSE)
# First we label each of the villages by their clusters
d$clusters <- c(1,1,2,2,3,3,4,4,5,5,6,6,7,7)
# Then we group by the clusters and greate a cluster level dataset
by_clusters <- d %>% group_by(clusters)
cluster.data <- by_clusters %>% summarise(cluster_Y = mean(Y), cluster_D = mean(D))
# Now define all the variables
k <- length(cluster.data$clusters) # number of clusters
N <- length(d$Village) # Total number of subjects
m <- 6 # size of treatment group (assume 3 clusters in treatment)
Ybar0 <- cluster.data$cluster_Y
Ybar1 <- cluster.data$cluster_D
# I created a function to calculate the variance the same way as equation 3.2
# This is because the var() function in R has N-1 in the denominator rather than N
variance <- function(data) {
var(data)*(length(data)-1)/length(data)
}
# A function is also created for covariance because the R version isn't what we want
covariance <- function(a,b) {
cov(a,b)*(length(a)+1)/length(a)
}
# Now to calcuate the SE with equation 3.22
SE.ATE <- sqrt((1/(k-1))*(m*variance(Ybar0)/(N-m)
+ (N-m)*variance(Ybar1)/m
+ 2*covariance(Ybar0, Ybar1)))
SE.ATE
# First we label each of the villages by their clusters
d$clusters <- c(1,2,3,4,5,6,7,7,6,5,4,3,2,1)
# Then we group by the clusters and greate a cluster level dataset
by_clusters <- d %>% group_by(clusters)
cluster.data <- by_clusters %>% summarise(cluster_Y = mean(Y), cluster_D = mean(D))
# Now define all the variables
k <- length(cluster.data$clusters) # number of clusters
N <- length(d$Village) # Total number of subjects
m <- 6 # size of treatment group (assume 3 clusters in treatment)
Ybar0 <- cluster.data$cluster_Y
Ybar1 <- cluster.data$cluster_D
# Now to calcuate the SE with equation 3.22
SE.ATE <- sqrt((1/(k-1))*(m*variance(Ybar0)/(N-m)
+ (N-m)*variance(Ybar1)/m
+ 2*covariance(Ybar0, Ybar1)))
SE.ATE
# Sample sizes in control and treatment groups
n1 <- 1000000/2
n2 <- 1000000/2
# Number of purchases
x1 <- 0.005*n1 # control
x2 <- (0.005 + 0.002)*n2 #treatment
# Calculate p
p <- (x1 + x2)/(n1 + n2)
# Calculate standard error for a two-sample proportion test
se <- sqrt(p*(1-p)*(1/n1 +1/n2))
# Calculate the left and right bound of the confidence interval
left.bound <- 0.002 - 1.96*se
right.bound <- 0.002 + 1.96*se
paste("The 95% confidence interval is (", left.bound,",",right.bound,")")
# Sample sizes in control and treatment groups
n1 <- 1000000 * 0.01 # control
n2 <- 1000000 * 0.99 # treatment
# Number of purchases
x1 <- 0.005*n1 # control
x2 <- (0.005 + 0.002)*n2 #treatment
# Calculate p
p <- (x1 + x2)/(n1 + n2)
# Calculate standard error for a two-sample proportion test
se <- sqrt(p*(1-p)*(1/n1 +1/n2))
# Calculate the left and right bound of the confidence interval
left.bound <- 0.002 - 1.96*se
right.bound <- 0.002 + 1.96*se
paste("The 95% confidence interval is (", left.bound,",",right.bound,")")
d2 <- read.csv("./data/listData.csv", stringsAsFactors = FALSE)
head(d2)
control <- d2$bid[d2$uniform_price_auction == 0]
treatment <- d2$bid[d2$uniform_price_auction == 1]
t.test(treatment,control)
# First create the regression
regression <- summary(lm(d2$bid~d2$uniform_price_auction))
regression
# Calculate the 95% confidence interval from the Regression
# Use slope +- (t-value)(std. error)
slope <- regression$coefficients[2,1]
Tval <- qt(0.975, 66) # using 66 dof
se <- regression$coefficients[2,2]
# Calculate the bounds
lower.bound <- slope - Tval*se
upper.bound <- slope + Tval*se
paste("The 95% confidence interval is (", lower.bound,",",upper.bound,")")
regression$coefficients[2,4]
# create function to run the simulation
simulation <- function(data) {
# First we randomly create assignments for treatment and control group.
# The number of 0's and 1's are not always equal but the probability of getting a 0
# is equal to the probability of getting a 1.
treatment <- sample(c(0,1), size = length(data$bid), replace = TRUE)
# Next we put the views data into their corresponding groups that we've randomly assigned
treat.group <- data$bid[treatment == 1]
cont.group <- data$bid[treatment == 0]
# Now we can calculate the estimated ATE for this specific random assignment of
# control/treatment groups.
ate <- mean(treat.group) - mean(cont.group)
return(ate)
}
# Now we run the simulation 10,000 times
distribution.under.sharp.null <- replicate(10000, simulation(d2))
# Now calculate the two-tail pvalue
pval <- sum(abs(distribution.under.sharp.null) >= 12.206)/10000
pval
# create function to run the simulation
simulation <- function(data) {
# First we randomly create assignments for treatment and control group.
# The number of 0's and 1's are not always equal but the probability of getting a 0
# is equal to the probability of getting a 1.
treatment <- sample(c(0,1), size = length(data$bid), replace = TRUE)
# Next we put the views data into their corresponding groups that we've randomly assigned
treat.group <- data$bid[treatment == 1]
cont.group <- data$bid[treatment == 0]
# Now we can calculate the estimated ATE for this specific random assignment of
# control/treatment groups.
ate <- mean(treat.group) - mean(cont.group)
return(ate)
}
# Now we run the simulation 10,000 times
distribution.under.sharp.null <- replicate(10000, simulation(d2))
# Now calculate the two-tail pvalue
pval <- sum(abs(distribution.under.sharp.null) >= 12.206)/10000
pval
paste("P-value from randomization inference is:", pval)
control <- d2$bid[d2$uniform_price_auction == 0]
treatment <- d2$bid[d2$uniform_price_auction == 1]
t.test(treatment,control)
library(data.table)
library(dplyr)
d <- read.csv("./data/Clingingsmith.2009.csv", stringsAsFactors = FALSE)
by_success <- d %>% group_by(success)
by_success %>% summarize(avg_views = mean(views), sample_size = length(views))
# Calculate the average treatment effect of the observed data
exp_ate <- mean(d[d$success == 1,]$views) - mean(d[d$success == 0,]$views)
exp_ate
# create function to run a simulation of random assignment under the sharp null hypothesis.
simulation <- function(data) {
# First we randomly create assignments for treatment and control group.
# The number of 0's and 1's are not always equal but the probability of getting a 0
# is equal to the probability of getting a 1.
treatment <- sample(c(0,1), size = length(data$views), replace = TRUE)
# Next we put the views data into their corresponding groups that we've randomly assigned
treat.group <- data$views[treatment == 1]
cont.group <- data$views[treatment == 0]
# Now we can calculate the estimated ATE for this specific random assignment of
# control/treatment groups.
ate <- mean(treat.group) - mean(cont.group)
return(ate)
}
# Now we run the simulation 10,000 times as directed
distribution.under.sharp.null <- replicate(10000, simulation(d))
# Plot the density distribution and histogram of our simulations
plot(density(distribution.under.sharp.null),
main = "Density under Sharp Null", xlab = "Simulated ATE")
abline(v = exp_ate, col = "red")
hist(distribution.under.sharp.null,
main = "Histogram under Sharp Null", xlab = "Simulated ATE")
abline(v = exp_ate, col = "red")
# The actual estimate of the ATE is stored as exp_ate
prob1c.answ <- sum(distribution.under.sharp.null >= exp_ate)
paste("Answer:", prob1c.answ)
prob1d.answ <- prob1c.answ/10000
paste("Implied one-tailed p-value:", prob1d.answ)
prob1e.answ <- sum(abs(distribution.under.sharp.null) >= exp_ate)
paste("Answer:", prob1e.answ)
prob1f.answ <- prob1e.answ/ 10000
paste("Implied two-tailed p-value:", prob1f.answ)
library(foreign)
d <- read.dta("./data/Titiunik.2010.dta")
head(d)
# First I will group the data by state to show the mean number of bills introduced.
by_state <- d %>% group_by(texas0_arkansas1)
by_state %>% summarise(avg_num_bills = mean(bills_introduced))
# Now I will group by two- or four-year terms to show the mean number of bills introduced.
by_term <- d %>% group_by(term2year)
by_term %>% summarise(avg_num_bills = mean(bills_introduced))
# Now I will group by both state and term length
by_state_term <- d %>% group_by(texas0_arkansas1, term2year)
blocked_dt <- by_state_term %>% summarise(avg_num_bills = mean(bills_introduced))
blocked_dt
# First calculate the ATE for each state.
tx.ate <- blocked_dt[2,3] - blocked_dt[1,3]
ar.ate <- blocked_dt[4,3] - blocked_dt[3,3]
paste("The ATE for the state of Texas is:", tx.ate)
paste("The ATE for the state of Arkansas is:", ar.ate)
# Method using equation 3.6 in Textbook
# First create 4 separete data tables for the control/treatment group of TX and AR.
tx.4year <- by_state_term %>% filter(texas0_arkansas1 == 0, term2year == 0)
tx.2year <- by_state_term %>% filter(texas0_arkansas1 == 0, term2year == 1)
ar.4year <- by_state_term %>% filter(texas0_arkansas1 == 1, term2year == 0)
ar.2year <- by_state_term %>% filter(texas0_arkansas1 == 1, term2year == 1)
# Calculate Var(Y(1)) using equation 3.7 for Texas
tx.4year.m <- length(tx.4year$bills_introduced)
tx.4year.mean <- sum(tx.4year$bills_introduced)/tx.4year.m
tx.var.Y0 <- sum((tx.4year$bills_introduced - tx.4year.mean)^2)/(tx.4year.m - 1)
# Calculate Var(Y(0)) using equation 3.8 for Texas
tx.2year.m <- length(tx.2year$bills_introduced)
tx.2year.mean <- sum(tx.2year$bills_introduced)/tx.2year.m
tx.var.Y1 <- sum((tx.2year$bills_introduced - tx.2year.mean)^2)/(tx.2year.m - 1)
# Calculate SE of the ATE using equation 3.6 for Texas
tx.SE.ate <- sqrt(tx.var.Y0/tx.4year.m + tx.var.Y1/tx.2year.m)
paste("The estimated SE of the ATE for Texas is", tx.SE.ate)
# Calculate Var(Y(1)) using equation 3.7 for Arkansas
ar.4year.m <- length(ar.4year$bills_introduced)
ar.4year.mean <- sum(ar.4year$bills_introduced)/ar.4year.m
ar.var.Y0 <- sum((ar.4year$bills_introduced - ar.4year.mean)^2)/(ar.4year.m - 1)
# Calculate Var(Y(0)) using equation 3.8 for Arkansas
ar.2year.m <- length(ar.2year$bills_introduced)
ar.2year.mean <- sum(ar.2year$bills_introduced)/ar.2year.m
ar.var.Y1 <- sum((ar.2year$bills_introduced - ar.2year.mean)^2)/(ar.2year.m - 1)
# Calculate SE of the ATE using equation 3.6 for Arkansas
ar.SE.ate <- sqrt(ar.var.Y0/ar.4year.m + ar.var.Y1/ar.2year.m)
paste("The estimated SE of the ATE for Arkansas is", ar.SE.ate)
# Method using regression
# First create a data table for only texas samples
tx.data <- by_state_term %>% filter(texas0_arkansas1 == 0)
# Do a linear regression
tx.regression <- summary(lm(tx.data$bills_introduced ~ tx.data$term2year))
# SE given in the regression summary
tx.se <- tx.regression$coefficients[2,2]
paste("The estimated SE of the ATE for Texas is", tx.se)
# First create a data table for only Arkansas samples
ar.data <- by_state_term %>% filter(texas0_arkansas1 == 1)
# Do a linear regression
ar.regression <- summary(lm(ar.data$bills_introduced ~ ar.data$term2year))
# SE given in the regression summary
ar.se <- ar.regression$coefficients[2,2]
paste("The estimated SE of the ATE for Arkansas is", ar.se)
tx.N <- length(tx.data$bills_introduced)
ar.N <- length(ar.data$bills_introduced)
N <- length(by_state_term$bills_introduced)
tx.ate <- as.numeric(tx.ate)
ar.ate <- as.numeric(ar.ate)
Overall.ATE <- (tx.N/N)*tx.ate + (ar.N/N)*ar.ate
Overall.ATE
# I will be using the standard errors obtained from the regression method
sqrt((tx.se^2)*(tx.N/N)^2 + (ar.se^2)*(ar.N/N)^2)
# create function to run a simulation of random assignment under the sharp null hypothesis.
simulation <- function(data) {
# First we randomly create assignments for treatment and control group.
# The number of 0's and 1's are not always equal but the probability of getting a 0
# is equal to the probability of getting a 1.
treatment <- sample(c(0,1), size = length(data$bills_introduced), replace = TRUE)
# Next we put the views data into their corresponding groups that we've randomly assigned
treat.group <- data$bills_introduced[treatment == 1]
cont.group <- data$bills_introduced[treatment == 0]
# Now we can calculate the estimated ATE for this specific random assignment of
# control/treatment groups.
ate <- mean(treat.group) - mean(cont.group)
return(ate)
}
# Now we create a function to find the overall ATEs for our simulations
Overall_ATE_Sim <- function(txdata, ardata) {
tx.sim.ate <- simulation(txdata)
ar.sim.ate <- simulation(ardata)
tx.N <- length(tx.data$bills_introduced)
ar.N <- length(ar.data$bills_introduced)
N <- length(by_state_term$bills_introduced)
OverallATE <- (tx.N/N)*tx.sim.ate + (ar.N/N)*ar.sim.ate
return(OverallATE)
}
# Now we run the simulation 10,000 times as directed
distribution.under.sharp.null <- replicate(10000, Overall_ATE_Sim(tx.data, ar.data))
# Plot the density distribution and histogram of our simulations
plot(density(distribution.under.sharp.null),
main = "Density under Sharp Null", xlab = "Simulated ATE")
abline(v = Overall.ATE, col = "red")
hist(distribution.under.sharp.null,
main = "Histogram under Sharp Null", xlab = "Simulated ATE")
abline(v = Overall.ATE, col = "red")
# Find the implied one-tailed p-value
onetail <- sum(distribution.under.sharp.null <= Overall.ATE)/10000
paste("Implied one-tailed p-value:", onetail)
#Find the implied two-tailed p-value
twotail <- sum(abs(distribution.under.sharp.null) >= abs(Overall.ATE))/10000
paste("Implied two-tailed p-value:", twotail)
layout(matrix(c(1,2,3,4), 2, 2, byrow = T))
hist(tx.2year$bills_introduced, main = "Texas, 2years (Treatment)", xlab = "# of Bills Introduced in 2003")
hist(tx.4year$bills_introduced, main = "Texas, 4years (Control)", xlab = "# of Bills Introduced in 2003")
hist(ar.2year$bills_introduced, main = "Arkansas, 2years (Treatment)", xlab = "# of Bills Introduced in 2003")
hist(ar.4year$bills_introduced, main = "Arkansas, 4years (Control)", xlab = "# of Bills Introduced in 2003")
## load data
d <- read.csv("./data/ggChapter3.csv", stringsAsFactors = FALSE)
# First we label each of the villages by their clusters
d$clusters <- c(1,1,2,2,3,3,4,4,5,5,6,6,7,7)
# Then we group by the clusters and greate a cluster level dataset
by_clusters <- d %>% group_by(clusters)
cluster.data <- by_clusters %>% summarise(cluster_Y = mean(Y), cluster_D = mean(D))
# Now define all the variables
k <- length(cluster.data$clusters) # number of clusters
N <- length(d$Village) # Total number of subjects
m <- 6 # size of treatment group (assume 3 clusters in treatment)
Ybar0 <- cluster.data$cluster_Y
Ybar1 <- cluster.data$cluster_D
# I created a function to calculate the variance the same way as equation 3.2
# This is because the var() function in R has N-1 in the denominator rather than N
variance <- function(data) {
var(data)*(length(data)-1)/length(data)
}
# A function is also created for covariance because the R version isn't what we want
covariance <- function(a,b) {
cov(a,b)*(length(a)-1)/length(a)
}
# Now to calcuate the SE with equation 3.22
SE.ATE <- sqrt((1/(k-1))*(m*variance(Ybar0)/(N-m)
+ (N-m)*variance(Ybar1)/m
+ 2*covariance(Ybar0, Ybar1)))
SE.ATE
# First we label each of the villages by their clusters
d$clusters <- c(1,2,3,4,5,6,7,7,6,5,4,3,2,1)
# Then we group by the clusters and greate a cluster level dataset
by_clusters <- d %>% group_by(clusters)
cluster.data <- by_clusters %>% summarise(cluster_Y = mean(Y), cluster_D = mean(D))
# Now define all the variables
k <- length(cluster.data$clusters) # number of clusters
N <- length(d$Village) # Total number of subjects
m <- 6 # size of treatment group (assume 3 clusters in treatment)
Ybar0 <- cluster.data$cluster_Y
Ybar1 <- cluster.data$cluster_D
# Now to calcuate the SE with equation 3.22
SE.ATE <- sqrt((1/(k-1))*(m*variance(Ybar0)/(N-m)
+ (N-m)*variance(Ybar1)/m
+ 2*covariance(Ybar0, Ybar1)))
SE.ATE
