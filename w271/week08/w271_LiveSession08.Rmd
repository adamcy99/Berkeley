---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data: Live Session 8'
author: "Professor Jeffrey Yau"
date: Spring 2019
output:
  pdf_document: default
  html_notebook: default
---

# Main Topics Covered in Lecture 8:

    - Autoregressive (AR) models
        - Lag (or backshift) operators
        - Properties of the general AR(p) model
        - Simulation of AR Models
        - Estimation, model diagnostics, model identification, model selection, assumption testing, and statistical inference
    - Moving Average (MA) Models
        - Lag (or backshift) operators
        - Mathematical formulation and derivation of key properties
        - Simulation of MA(q) models
        - Estimation, model diagnostics, model identification, model selection, assumption testing, and statistical inference / forecasting


# Readings:

**CM2009:** Paul S.P. Cowpertwait and Andrew V. Metcalfe. *Introductory Time Series with R*. Springer. 2009. 

    - Ch. 3.1, 3.2, 4.5, 6.1 â€“ 6.4

**HA:** Rob J Hyndman and George Athanasopoulos. Forecasting: Principles and Practice.

    - 8.2, 8.3, 8.4

# Agenda for the Live Session

The focus of this live session is entirely on reviewing the steps to build *AR(p)* models and discussing the relevant concepts, leaving *MA(q)* models as take-home exericses.

  1. Quiz

  2. Overivew of the lecture
  
  3. Recap of various important concepts
  
  4. Go through the practical aspects of buiding an AR model using an example

  5. Hands-on break-out room exercise to repeat the practical aspects of buiding an AR model

# Lecture Overview

In Lecture 7, we discussed applying classical regression to time series data, including the use of various trend regression, seasonality dummy regression, a number of smoothing techniques, and exploratory data analysis for time series data, which requires the use of visuals displaying dynamics that are not visible under histogram or, for that matter, density plots, as they don't capture the time element.

In addition to various graphical techniques used to examine and identify the key patterns of time series data, we also learned about a couple of in-sample model performance measurements - AIC and BIC - and how to measure dependency structures using autocorrelation functions (i.e. correlogram), partial autocorrelation functions, the notion of stationarity, and how to spot check them through graphs. Furthermore, we discussed how to simulate time series using the most fundamental time series models - linear and other deterministic trends, white noise, moving averges, autoregressive models, and random walk (with and without drift). These are important techniques to understand the kinds of patterns that can be generated underlying these fundamental stochastic processes. However, the simulation is often undervalued by the students who are quick to dismiss them as useless simply because the simulated data are not "real-world" data. 

Classical linear regression models, however, are insufficient for explaining all of the interesting dynamics of a time series, meaning that there could be additional structure of the data that is not captured.

In this lecture, we will study in-depth autoregressive models and moving-average models, both of which are the essential building blocks for the more general class of mixed autoregressive moving integrated average models. We will learn about identification of the order of dependency in AR and MA models using ACF and PACF,estimation, diagonsis of residuals (after the model is estimated), model assumption testing, model performance evaluation, and
forecasting.
 
We will also start to learn about and use extensively the *principle of parsimonious* in building time series models and will continue to develop this important principle in the next two lectures.
 
It is very important to keep in mind that this class of models applies only to **stationary processes**. Therefore, we always need to check for stationarity before applying AR(p) models to the data.

Finally, note that I used the *ar()* function in the async lecture in order to follow Cowpertwait and Metcalfe's *Introductory Time Series with R*, I will introduce both *arima()*, which comes with the *stats* package, and *Arima()*, which comes with the *forecast* package.

[arima](http://stat.ethz.ch/R-manual/R-devel/library/stats/html/arima.html)
[forecast](https://cran.r-project.org/web/packages/forecast/forecast.pdf)
[acf and pacf](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/acf.html)

# Stationary Time Series Models
A stationary time series is one whose statistical moments is not a function of time. As such,time series with trends or seasonality are not stationary; the trend and seasonality will affect the value of the time series at different times. 

However, the stationary requirement does not exclude the case of cyclic behavior that doesn't have trend or seasonality.  Cycles do not have a fixed length, so before the series is observed, the peaks and troughs of the cycles cannot be determined with certainty.



# Recap of AIC and BIC

**Akaike Information Criterion (AIC)**

$AIC = -2 \times logL_k + 2 \times k$
where $logL_k$ is the maximized log-likelihood and $k$ is the number of parameters in the model.

One could normalize it by $n$, the number of observation used to estimate the model, and obtain

$$
  AIC = \frac{-2logL_k + 2k}{n} \approx ln(\hat{\sigma}^2) + \frac{2k}{n} + c
$$
where $\hat{\sigma}^2$ denotes the MLE of $\sigma^2$ and $c$ is some constant.

**Bayesian Information Criteria (BIC)**

This allows one to compare with another commonly used information criteria, *Bayesian Information Criteria*:

$$
  BIC = ln(\hat{\sigma}^2) + k\frac{ln(n)}{n}
$$

  * Note that BIC imposes a greater penalty for the number of estimated model parameters than does AIC. As such, BIC would always gives a model whose number of parameters is no greater than that chosen under AIC.

  * Information criteriion model selection process should NOT be use as a substitute for careful examination of characteristics of the estimated autocorrelation and partial autocorrelation; it can be used as a supplemenatry guidelines.

  * Critial examination of the residual series for model inadequacies should always be included as a major aspect of the overall model selection process.

# ARIMA Modeling Procedure Recap

When fitting the class of ARIMA model to a set of time series data, the following procedure provides a useful general approach.

  1. Plot the data. Identify any unusual observations.

  (SKIP IN THIS LECTURE) 2. If necessary, transform the data (using a Box-Cox transformation) to stabilize the variance.

  (SKIP IN THIS LECTURE) 3. If the data are non-stationary: take first differences of the data until the data are stationary.

  4. Examine the ACF/PACF: Is an AR(p) or MA(q) model appropriate?
 
  5. Try your chosen model(s), and use appropriate metrics to choose a model.

  6. Model evaluation Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.

  7. Once the residuals look like white noise, calculate forecasts.

# Concepts related to AR processes
  

# An Example      
Start-up Codes:
```{r global_options, include=FALSE}
#knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
#                      echo=FALSE, warning=FALSE, message=FALSE)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE,warning=FALSE, message=FALSE)
```

```{r}
# Clean up the workspace before we begin
rm(list = ls())

# Load required libraries
library(car)
library(dplyr)
library(Hmisc)
library(ggplot2)
library(ggfortify)
library(plotly)
library(astsa)
library(forecast)
library(fpp2)

# Set working directory
wd = "~/Documents/Teach/Cal/w271/course-main-dev/live-session-files/week08"

setwd(wd)
```

Load into R the given data series, *series1.csv*, which is a monthly series starting in 2005 January. The series has been modified from its original series (for the practices in this live session). 

```{r}
df <- read.csv("series1.csv", header = FALSE, sep=",")

# Examine the data structure
str(df)
names(df)
head(df)
tail(df)

# Convert it into a time serie object
series1 <- ts(df$V1, start=c(2005,1), frequency = 12)

# Examine the converted data structure
str(series1)
head(cbind(time(series1),series1))
```

# Exploratory Time Series Data Analysis

```{r}
par(mfrow=c(2,2))
plot(series1, main = "A Given Monthly Time Series")
hist(series1)
acf(series1, main="ACF of Series 1")
pacf(series1, main="PACF of Series 1")
boxplot(series1 ~ cycle(series1), main="Series 1")
```

```{r}
library(gridExtra)
library(grid)

#autoplot(acf(series1, plot = FALSE)) +
#  ggtitle("ACF of A Given Monthly Time Series")
#ggplotly()

p1 = ggplot(series1, 
       aes(x=time(series1), y=series1)) + 
  geom_line(colour = "navy", size = 1) +
  ggtitle("A Given Montly Time Series") +
  theme(axis.title = element_text(size = rel(1.5)))

p2 = ggplot(df, aes(x=V1)) +
  geom_histogram(aes(fill = ..count..)) +
  ggtitle("A Given Monthly Time Series") +
  xlab("Series 1") + ylab("Frequency")

p3 = autoplot(acf(series1, plot = FALSE)) +
  ggtitle("ACF of A Given Monthly Time Series")

p4 = autoplot(pacf(series1, plot = FALSE)) +
  ggtitle("PACF of A Given Monthly Time Series")

grid.arrange(p1, p2, p3, p4, ncol=2)
```

# An Introducion to the arima() function, Model Estimation, Model Selection, and Model Checking / Diagnostic Analysis

An Introducion to the `arima()` function with an excerpt from R documentation:

```
arima(x, order = c(0L, 0L, 0L),
      seasonal = list(order = c(0L, 0L, 0L), period = NA),
      xreg = NULL, include.mean = TRUE,
      transform.pars = TRUE,
      fixed = NULL, init = NULL,
      method = c("CSS-ML", "ML", "CSS"), n.cond,
      SSinit = c("Gardner1980", "Rossignol2011"),
      optim.method = "BFGS",
      optim.control = list(), kappa = 1e6)

selected arguments:
x - a univariate time series

order - A specification of the non-seasonal part of the ARIMA model: the three integer components (p, d, q) are the AR order, the degree of differencing, and the MA order.

seasonal - A specification of the seasonal part of the ARIMA model, plus the period (which defaults to frequency(x)). This should be a list with components order and period, but a specification of just a numeric vector of length 3 will be turned into a suitable list with the specification as the order.

xreg - Optionally, a vector or matrix of external regressors, which must have the same number of rows as x.

include.mean - should the ARMA model include a mean/intercept term? The default is TRUE for undifferenced series, and it is ignored for ARIMA models with differencing.
```

```{r}

# I started with AR(3) based on the patterns exhibited in acf and pacf
ar.fit1 <- arima(series1, c(ar=3,0,0)) 
summary(ar.fit1)
str(ar.fit1)

# Model Selection -  Which model is selected based on the AIC?
# Note that auto.arima(), by default, select model based on AIC
library(forecast)
auto.arima(series1)

# Examine the roots of the Characteristics equation
plot(ar.fit1)
Mod(polyroot(c(ar.fit1$coef)))


# Model Evaluation (Residual Diagnostic)
summary(ar.fit1$resid)
str(ar.fit1$resid)
plot(ar.fit1$resid)

par(mfrow=c(2,2))
plot(rnorm(120), type="l", main="Gaussian White Noise")
plot(ar.fit1$resid[-c(1:2)], type="l", main="Residuals: t-plot")
hist(ar.fit1$resid)
acf(ar.fit1$resid[-c(1:2)], main="ACF of the Residual Series")
pacf(ar.fit1$resid[-c(1:2)], main="ACF of the Residual Series")

# See https://www.rdocumentation.org/packages/WeightedPortTest/versions/1.0/topics/Weighted.Box.test
Box.test(residuals(ar.fit1), lag=24, type="Ljung")

tail(cbind(time(series1),series1))
ar.fit1.forecast = forecast(ar.fit1, h=12)
plot(ar.fit1.forecast)
```

# Break-out Session: Your Turn (Repeat what I just did using `series2.csv`)

  1. Load `series2.csv`
  2. Examine basic structure of the series 
  3. Convert it into a `ts()` structure
  4. Examine the strucutre again after the conversion
  5. Conduct time series EDA of the series
  6. Based on your time series EDA, does the series look more or less like a stationary series?  Does AR and/or MA model a good starting point?
  7. If your answerr to question 6 is yes, then determine a good starting order and estimate an AR or a MA model. Perhaps try a couple other models with different orders.
  8. After model estimation, compare the model using your evaluation metrics.
  9. Conduct model diagnostics
  10. Conduct a 12-step ahead forecast


```{r}
df <- read.csv("series2.csv", header = FALSE, sep=",")

# Examine the data structure
str(df)
names(df)
head(df)
tail(df)

# Convert it into a time serie object
series2 <- ts(df$V1, start=c(2005,1), frequency = 12)

# Examine the converted data structure
str(series2)
head(cbind(time(series2),series2))

library(gridExtra)
library(grid)

#autoplot(acf(series1, plot = FALSE)) +
#  ggtitle("ACF of A Given Monthly Time Series")
#ggplotly()

p1 = ggplot(series2, 
       aes(x=time(series2), y=series2)) + 
  geom_line(colour = "navy", size = 1) +
  ggtitle("A Given Montly Time Series") +
  theme(axis.title = element_text(size = rel(1.5)))

p2 = ggplot(df, aes(x=V1)) +
  geom_histogram(aes(fill = ..count..)) +
  ggtitle("A Given Monthly Time Series") +
  xlab("Series 1") + ylab("Frequency")

p3 = autoplot(acf(series2, plot = FALSE)) +
  ggtitle("ACF of A Given Monthly Time Series")

p4 = autoplot(pacf(series2, plot = FALSE)) +
  ggtitle("PACF of A Given Monthly Time Series")

grid.arrange(p1, p2, p3, p4, ncol=2)

# I started with AR(3) based on the patterns exhibited in acf and pacf
ma.fit1 <- arima(series2, c(0,0,3)) 
summary(ma.fit1)
str(ma.fit1)

# Model Selection -  Which model is selected based on the AIC?
library(forecast)
auto.arima(series2)

# Model Evaluation (Residual Diagnostic)
summary(ma.fit1$resid)
str(ma.fit1$resid)
plot(ma.fit1$resid)

par(mfrow=c(2,2))
#plot(rnorm(120), type="l", main="Gaussian White Noise")
plot(ma.fit1$resid[-c(1:2)], type="l", main="Residuals: t-plot")
hist(ma.fit1$resid)
acf(ma.fit1$resid[-c(1:2)], main="ACF of the Residual Series")
pacf(ma.fit1$resid[-c(1:2)], main="ACF of the Residual Series")
qqPlot(ma.fit1$residuals)

# See https://www.rdocumentation.org/packages/WeightedPortTest/versions/1.0/topics/Weighted.Box.test
Box.test(residuals(ma.fit1), lag=24, type="Ljung")

tail(cbind(time(series2),series2)) 
ma.fit1.forecast = forecast(ma.fit1, h=12)
plot(ma.fit1.forecast)
```